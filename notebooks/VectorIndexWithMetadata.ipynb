{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Iterator, Optional\n",
    "import openai\n",
    "\n",
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "from llama_index.callbacks.base import CallbackManager\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import chainlit as cl\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.chat_engine.types import BaseChatEngine, ChatMode\n",
    "\n",
    "from llama_index import (\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    set_global_service_context,\n",
    ")\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser.extractors import (\n",
    "    MetadataExtractor,\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    EntityExtractor,\n",
    ")\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.schema import MetadataMode\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain gpt model name from environment variables\n",
    "gpt_model = \"gpt-4\"\n",
    "gpt_temperature = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = [\n",
    "\t'../quant_scraper/docs/vbt_pro/cookbook.md',\n",
    "\t'../quant_scraper/docs/vbt_pro/documentation.md',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_indexer = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\", max_tokens=512)\n",
    "\n",
    "text_splitter = TokenTextSplitter(separator=\"\\n## \", chunk_size=1024, chunk_overlap=0)\n",
    "\n",
    "metadata_extractor = MetadataExtractor(\n",
    "    extractors=[\n",
    "        # TitleExtractor(nodes=3, llm=llm_indexer),\n",
    "        # KeywordExtractor(keywords=3, llm=llm_indexer),\n",
    "        # EntityExtractor(prediction_threshold=0.5, llm=llm_indexer),\n",
    "        # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm_indexer),\n",
    "        QuestionsAnsweredExtractor(questions=5, llm=llm_indexer),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=text_splitter,\n",
    "    # metadata_extractor=metadata_extractor,\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=document_list).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents into nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:00<00:00, 404.19it/s]\n"
     ]
    }
   ],
   "source": [
    "index_nodes = node_parser.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookbook\n",
      "\n",
      "This is a repository for short and sweet examples and links for useful\n",
      "VectorBT PRO recipes. Simplified, condensed, new-user friendly, in-line\n",
      "examples have been inserted where possible to augment the tutorials, the\n",
      "documentation, and the API. We encourage users to add to this documentation.\n",
      "\n",
      "Imports required by the code examples\n",
      "\n",
      "    \n",
      "    \n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    from numba import njit\n",
      "    import vectorbtpro as vbt\n",
      "Summary\n",
      "\n",
      "Kudos for following me all the way down here! The classes that we just covered\n",
      "build a strong foundation for data analysis with vectorbt; they implement\n",
      "design patterns that are encountered in most other places across the codebase,\n",
      "which makes them very easy to recognize and extend. In fact, the most hard-\n",
      "core class\n",
      "Portfolio\n",
      "is very similar to our `CorrStats`.\n",
      "\n",
      "You're now more than ready for using vectorbt, soldier\n",
      "!ðŸŒŸ\n",
      "\n",
      "[ Python\n",
      "code](https://vectorbt.pro/pvt_6d299575/assets/jupytext/documentation/building-\n",
      "blocks.py.txt)\n"
     ]
    }
   ],
   "source": [
    "print(index_nodes[0].text)\n",
    "print(index_nodes[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all unique metadata entities fields\n",
    "unique_entities = set()\n",
    "for node in index_nodes:\n",
    "\tprint(node.metadata['questions_this_excerpt_can_answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes=index_nodes,\n",
    "    service_context=ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-4\")),\n",
    "\tshow_progress=True,\n",
    ")\n",
    "\n",
    "index.storage_context.persist(\"../index_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ]\n",
    ")\n",
    "\n",
    "final_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        QueryEngineTool(\n",
    "            query_engine=engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"quant_knowledge_base\",\n",
    "                description=\"technical documentation for vectorbt pro\",\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"\"\"\n",
    "In my strategy I have 4 take profits and stop loss. How can I move stop loss to breakeven after the first take profit is hit?\n",
    "\"\"\"\n",
    "\n",
    "# result = await final_engine.aquery(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_llm = LLMPredictor(\n",
    "\tllm=ChatOpenAI(\n",
    "\t\ttemperature=gpt_temperature,\n",
    "\t\tmodel_name=gpt_model,\n",
    "\t\tmax_tokens=2048,\n",
    "\t\tstreaming=True,\n",
    "\t),\n",
    ")\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=final_llm,\n",
    "    chunk_size=1024,\n",
    ")\n",
    "\n",
    "\n",
    "index2 = VectorStoreIndex(\n",
    "    nodes=index_nodes,\n",
    "    service_context=service_context,\n",
    "\tshow_progress=True,\n",
    ")\n",
    "\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "\tresponse_mode=\"tree_summarize\", service_context=service_context)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "\tindex=index,\n",
    "\tsimilarity_top_k=10,\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine2 = RetrieverQueryEngine.from_args(\n",
    "\tstreaming=True,\n",
    "\tretriever=retriever,\n",
    "\tresponse_synthesizer=response_synthesizer,\n",
    "\tservice_context=service_context,\n",
    "\tnode_postprocessors=[\n",
    "\t\tSimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await query_engine2.aquery(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate all result.source_nodes and print score\n",
    "\n",
    "for node in result.source_nodes:\n",
    "\tprint(node.score)\n",
    "\tprint(node.metadata['questions_this_excerpt_can_answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "\n",
    "# import chainlit as cl\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = [\n",
    "\t'../quant_scraper/docs/vbt_pro/cookbook-persistence.txt',\n",
    "\t'../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefaultVectorStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_embed = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "llm_chat = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0.1)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "\tllm=llm_chat,\n",
    "\tchunk_size=4096,\n",
    "\tembed_model=llm_embed,\n",
    "\t# callback_manager=CallbackManager([cl.LlamaIndexCallbackHandler()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading documents...')\n",
    "# documents = SimpleDirectoryReader(input_dir='../quant_scraper/docs/', recursive=True).load_data(num_workers=4, show_progress=True)\n",
    "documents = SimpleDirectoryReader(input_files=document_list).load_data(num_workers=4, show_progress=True)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = SentenceSplitter(chunk_size=2048, chunk_overlap=0, separator=\"\\n\\n#  \")\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 58.85it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Building index...')\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resposne = index.as_chat_engine().chat('overview vbt pro cookbook')\n",
    "# print(resposne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "\tindex=index,\n",
    "\tsimilarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "\tresponse_mode=\"tree_summarize\", service_context=service_context)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.as_query_engine().query(\"How do I use the vbt pro library?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement a strategy where the stop loss is moved to breakeven after the first take profit is hit, you can follow a structured approach. This involves tracking the status of your take profits and adjusting the stop loss accordingly. Here's a general outline of how you might achieve this:\n",
      "\n",
      "1. **Define Your Entry, Take Profit, and Stop Loss Levels**: Initially, you need to define where your entry point is, where your take profits are located, and where your initial stop loss is placed. The take profits can be defined as a percentage or price levels above your entry point, and the stop loss can be a percentage or price level below your entry point.\n",
      "\n",
      "2. **Track the First Take Profit Hit**: You need a mechanism to track when your first take profit level is hit. This can be done by monitoring the price and comparing it with your predefined take profit levels.\n",
      "\n",
      "3. **Move Stop Loss to Breakeven**: Once the first take profit is hit, you adjust your stop loss to the breakeven point, which is typically your entry price. This means if the price then reverses to this level, you exit the trade without a loss (excluding transaction costs).\n",
      "\n",
      "4. **Implement Logic in Code**: If you're automating this strategy, you'll need to implement the logic in code. This involves setting up conditions to check if the first take profit has been hit and then adjusting the stop loss accordingly.\n",
      "\n",
      "Here's a pseudo-code example to illustrate the concept:\n",
      "\n",
      "```python\n",
      "entry_price = 100\n",
      "take_profits = [105, 110, 115, 120]  # Example take profit levels\n",
      "initial_stop_loss = 95\n",
      "stop_loss = initial_stop_loss\n",
      "\n",
      "# Function to simulate price update\n",
      "def on_price_update(current_price):\n",
      "    global stop_loss\n",
      "    # Check if the first take profit is hit\n",
      "    if current_price >= take_profits[0]:\n",
      "        # Move stop loss to breakeven\n",
      "        stop_loss = entry_price\n",
      "        print(f\"First take profit hit. Stop loss moved to breakeven at {stop_loss}.\")\n",
      "    # Additional logic to handle other take profits or stop loss hit\n",
      "    # ...\n",
      "\n",
      "# Simulate price movement\n",
      "price_updates = [101, 102, 106, 107, 104]  # Example price updates\n",
      "for price in price_updates:\n",
      "    on_price_update(price)\n",
      "```\n",
      "\n",
      "This example assumes you have a mechanism to receive or simulate price updates (`on_price_update` function) and adjust your stop loss accordingly. In a live trading environment, this logic would be part of your trading bot or platform that supports custom scripting and order management.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"\"\"\n",
    "In my strategy I have 4 take profits and stop loss. How can I move stop loss to breakeven after the first take profit is hit?\n",
    "\"\"\"\n",
    "\n",
    "response = query_engine.query(test_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='68d0cdf5-4284-42ee-ad0d-44e531f2e967', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/cookbook-persistence.txt', 'file_name': 'cookbook-persistence.txt', 'file_type': 'text/plain', 'file_size': 3968, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='76f89382-71ad-4766-9278-5bc0b5fe5a00', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/cookbook-persistence.txt', 'file_name': 'cookbook-persistence.txt', 'file_type': 'text/plain', 'file_size': 3968, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='cfa9f4ed0c291f348860dea229f8991681af407a2db15b111589f7e1699f7920'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4e21ccac-4641-46a4-864e-4f722ee20b1d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='dd46af30849aa5c66ef17bff9d664dcb3152503f83d435591d0c23d373dcfc20')}, text='#  Persistence\\n\\nAny Python object can be serialized and saved to disk as a pickle file with\\n[save](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.save).\\n\\nSave a dict to a file\\n\\n    \\n    \\n    cache = dict(data=data, indicator=indicator, pf=pf)\\n    vbt.save(cache, \"cache.pickle\")\\n    \\n\\nImportant\\n\\nIf a file with the same name already exists, it will be overridden.\\n\\nA pickle file can then be loaded back and deserialized with\\n[load](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.load).\\n\\nLoad the dict back\\n\\n    \\n    \\n    cache = vbt.load(\"cache.pickle\")\\n    \\n\\nNote\\n\\nThe file can be read in another Python environment and even on another machine\\n(such as in cloud), just make sure that the Python and package versions on\\nboth ends are approximately the same.\\n\\nPickle files usually take a considerable amount of space, to reduce it\\ncompression can be used. The most recommended compression algorithm for binary\\nfiles is [blosc](https://github.com/Blosc/c-blosc). To later load the\\ncompressed file, pass the `compression` argument in the exact same way to the\\nloader, or simply append the \".blosc\" extension to the filename for the loader\\nto recognize it automatically. The supported algorithms and their possible\\nextensions are listed under `extensions` in\\n[settings.pickling](https://vectorbt.pro/pvt_6739a1da/api/_settings/#vectorbtpro._settings.pickling).\\n\\nSpecify the compression explicitly\\n\\n    \\n    \\n    vbt.save(cache, \"cache.pickle\", compression=\"blosc\")\\n    cache = vbt.load(\"cache.pickle\", compression=\"blosc\")\\n    \\n\\nSpecify the compression implicitly\\n\\n    \\n    \\n    vbt.save(cache, \"cache.pickle.blosc\")\\n    cache = vbt.load(\"cache.pickle.blosc\")\\n    \\n\\nThose VBT objects that subclass\\n[Pickleable](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable)\\ncan also be saved individually. Benefit: the name of the class and optionally\\nthe compression algorithm will be packed into the filename by default to\\nsimplify loading. The object can be loaded back using the `load()` method of\\nthe object\\'s class.\\n\\nSave a portfolio under \\'Portfolio.pickle.blosc\\' and load it back\\n\\n    \\n    \\n    pf.save(compression=\"blosc\")\\n    pf = vbt.PF.load()\\n    \\n\\nIf a VBT object was saved with an older package version and upon loading with\\na newer version an error is thrown (for example, due to a different order of\\nthe arguments), the object can still be reconstructed by creating and\\nregistering a\\n[RecInfo](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.RecInfo)\\ninstance before loading.\\n\\nReconstruct an older BinanceData instance\\n\\n    \\n    \\n    def modify_state(rec_state):  # (1)!\\n        return vbt.RecState(\\n            init_args=rec_state.init_args,\\n            init_kwargs=rec_state.init_kwargs,\\n            attr_dct=rec_state.attr_dct,\\n        )\\n    \\n    rec_info = vbt.RecInfo(\\n        vbt.get_id_from_class(vbt.BinanceData),\\n        vbt.BinanceData,\\n        modify_state\\n    )\\n    rec_info.register()\\n    data = vbt.BinanceData.load()\\n    \\n\\n  1. Change the arguments to be passed to the class here\\n\\nIf there are issues with saving an instance of a specific class, set the\\nreconstruction id `_rec_id` with any string and then reconstruct the object\\nusing this id (first argument of `RecInfo`).\\n\\nSet a custom identifier to a class and reconstruct its instance using another\\nclass\\n\\n    \\n    \\n    class MyClass1(vbt.Configured):  # (1)!\\n        _rec_id = \"MyClass\"\\n        ...\\n    \\n    my_obj = MyClass1()\\n    vbt.save(my_obj, \"my_obj\")\\n    \\n    # (2)!\\n    \\n    class MyClass2(vbt.Configured):\\n        ...\\n    \\n    rec_info = vbt.RecInfo(\"MyClass\", MyClass2)\\n    rec_info.register()\\n    my_obj = vbt.load(\"my_obj\")  # (3)!\\n    \\n\\n  1. `vbt.Configured` can be replaced by any other VBT class that subclasses it\\n  2. ...restart the kernel...\\n  3. The resulting object is of the class `MyClass2` even though we saved it as `MyClass1`!', start_char_idx=0, end_char_idx=3966, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.21441312924015904),\n",
       " NodeWithScore(node=TextNode(id_='7f856c6e-02ee-4d7d-bc80-c1499f8878dd', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='23a68867-577b-45cd-8c81-324775b1b92b', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='4a572a8f360d1d09082cb7e16863d79c1c943beb21ed4e3c034b62abc52b9ae5')}, text='...         corr=dict(\\n    ...             title=\\'Corr. Coefficient\\',\\n    ...             calc_func=\\'corr\\'\\n    ...         )\\n    ...     )\\n    ...\\n    ...     _subplots = vbt.HybridConfig(  # (4)!\\n    ...          rolling_corr=dict(\\n    ...              title=vbt.Sub(\"Rolling Corr. Coefficient (window=$window)\"),\\n    ...              plot_func=vbt.Sub(\\'rolling_corr($window).vbt.plot\\'),  # (5)!\\n    ...              pass_trace_names=False\\n    ...          )\\n    ...     )\\n    \\n\\n  1. Replace `Wrapping` with `Analyzable`\\n  2. Replace `Wrapping` with `Analyzable`\\n  3. Define default metrics for [StatsBuilderMixin.stats](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)\\n  4. Define default subplots for [PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)\\n  5. Recall Templating and Attribute resolution?\\n\\nWe changed a few things: replaced `Wrapping` with `Analyzable`, and added some\\nmetrics and subplots based on `CorrStats.corr` and `CorrStats.rolling_corr`.\\nThat\\'s it! We can now pass arbitrary array-like objects to\\n`CorrStats.from_objs` and it will return an instance that can be used to\\nanalyze the correlation between the objects, in particular using\\n`CorrStats.stats` and `CorrStats.plots`:\\n\\n    \\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2)\\n    >>> corrstats.stats(column=\\'a\\')  # (1)!\\n    Corr. Coefficient    0.3\\n    Name: a, dtype: object\\n    \\n    >>> corrstats[\\'a\\'].stats()  # (2)!\\n    Corr. Coefficient    0.3\\n    Name: a, dtype: object\\n    \\n    >>> corrstats.plots(template_context=dict(window=3)).show()  # (3)!\\n    \\n\\n  1. Compute metrics for all columns and display only `a`\\n  2. Compute and display metrics only for `a`\\n  3. Set the rolling window to 3 and display subplots for all columns\\n\\n![](https://vectorbt.pro/pvt_6739a1da/assets/images/documentation/bblocks/analyzing.svg)\\n\\nThere is nothing more satisfying than not having to write boilerplate code.\\nThanks to\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),\\nwe can shift our focus entirely to analysis, while vectorbt takes care of\\neverything else.\\n\\n### Generic accessor\\n\\nWe don\\'t have to look far to find a class that inherits\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable):\\nclass\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)\\nextends the class\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nto deliver statistics and plots for any numeric data. It\\'s a size-that-fits-\\nall class with an objective to replicate, accelerate, and extend Pandas\\' core\\nfunctionality. It implements in-house rolling, mapping, reducing, splitting,\\nplotting, and many other kinds of methods, which can be used on any Series or\\nDataFrame in the wild.\\n\\nIn a nutshell,\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)\\ndoes the following:\\n\\n  * Wraps many Numba-compiled functions from [nb](https://vectorbt.pro/pvt_6739a1da/api/generic/nb/) module and offers them as methods that mimic some of the Pandas most popular functions. Some of them have meta versions (accepting UDFs that take metadata instead of arrays) or can work on grouped data.\\n  * Wraps many data transformation utilities from [scikit-learn](https://scikit-learn.org/stable/)\\n  * Displays a range of statistics similar to [pandas.DataFrame.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)\\n  * Wraps widgets from [plotting](https://vectorbt.pro/pvt_6739a1da/api/generic/plotting/) and provides many custom plotting methods. Some of them even support interactive controls for analyzing groups of data.\\n  * Extracts basic events in form of [Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records), such as drawdowns\\n\\nSimilarly to the Base accessor, the generic accessor uses [class\\ndecorators](https://vectorbt.pro/pvt_6739a1da/api/generic/decorators/) and\\n[configs](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.nb_config)\\nto attach many Numba-compiled and scikit-learn functions at once.\\n\\nUsage is similar to `CorrStats`, except that we can call the generic accessor\\ndirectly on Pandas objects since it\\'s being directly subclassed by\\n[Vbt_DFAccessor](https://vectorbt.pro/pvt_6739a1da/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)!\\n\\n    \\n    \\n    >>> df.vbt.stats(column=\\'a\\')\\n    Start        2020-01-01 00:00:00\\n    End          2020-01-05 00:00:00\\n    Period           5 days 00:00:00\\n    Count                          5\\n    Mean                         2.0\\n    Std                     1.581139\\n    Min                            0\\n    Median                       2.0\\n    Max                            4\\n    Min Index    2020-01-01 00:00:00\\n    Max Index    2020-01-05 00:00:00\\n    Name: a, dtype: object\\n    \\n\\n## Records\\n\\n    \\n    \\n    flowchart TD;\\n        Records -.->|references| ColumnMapper;\\n        Records -.->|references| MappedArray;\\n        Records -->|inherits| Analyzable;\\n        MappedArray -.->|references| ColumnMapper;\\n        MappedArray -->|inherits| Analyzable;\\n\\nRecords are [structured\\narrays](https://numpy.org/doc/stable/user/basics.rec.html) \\\\- a NumPy array\\nthat can hold different data types, just like a Pandas DataFrame. Records have\\none big advantage over DataFrames though: they are well understood by Numba,\\nthus we can generate and use them efficiently. So, what\\'s the catch? Records\\nhave no (index) labels and the API is very limited. We also\\n[learned](https://vectorbt.pro/pvt_6739a1da/documentation/fundamentals/) that\\nvectorbt doesn\\'t like heterogenous data and prefers to work with multiple\\nhomogeneous arrays (remember how we need to split OHLC into O, H, L, and C?).\\nNevertheless, they take a very important place in our ecosystem - as\\ncontainers for event data.\\n\\nTrading is all about events: executing trades, aggregating them into\\npositions, analyzing drawdowns, and much more. Each of such events is a\\ncomplex piece of data that needs a container optimized for fast writes and\\nreads, especially inside a Numba-compiled code (but please not a list of\\ndictionaries, they are **very inefficient** ). Structured arrays is exactly\\nthe data structure we need! Each event is a record that holds all the required\\ninformation, such the column and row where it originally happened.\\n\\nBecause structured arrays are hard to analyze, there is a class just for this\\n-\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)!\\nBy subclassing\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)\\n(suprise, surprise), it wraps a structured NumPy array and provides us with\\nuseful tools for its analysis. Every\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)\\ninstance can be indexed just like a regular Pandas object and compute various\\nmetrics and plot graphs.\\n\\nLet\\'s generate the\\n[Drawdowns](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)\\nrecords for two columns of time series data:\\n\\n    \\n    \\n    >>> dd_df = pd.DataFrame({\\n    ...     \\'a\\': [10, 11, 12, 11, 12, 13, 12],\\n    ...     \\'b\\': [14, 13, 12, 11, 12, 13, 14]\\n    ... }, index=[datetime(2020, 1, 1) + timedelta(days=i) for i in range(7)])\\n    >>> drawdowns = dd_df.vbt.drawdowns\\n    >>> drawdowns.records_readable\\n       Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\\\\\\\\\n    0            0      a     2020-01-03      2020-01-04       2020-01-04   \\n    1            1      a     2020-01-06      2020-01-07       2020-01-07   \\n    2            0      b     2020-01-01      2020-01-02       2020-01-04   \\n    \\n      End Timestamp  Peak Value  Valley Value  End Value     Status  \\n    0    2020-01-05        12.0          11.0       12.0  Recovered  \\n    1    2020-01-07        13.0          12.0       12.0     Active  \\n    2    2020-01-07        14.0          11.0       14.0  Recovered \\n    \\n    >>> drawdowns[\\'b\\'].records_readable  # (1)!\\n       Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\\\\\\\\\n    0            0      b     2020-01-01      2020-01-02       2020-01-04   \\n    \\n      End Timestamp  Peak Value  Valley Value  End Value     Status  \\n    0    2020-01-07        14.0          11.0       14.0  Recovered\\n    \\n\\n  1. Select all records of the column `b` and display them in a human-readable format\\n\\nThat\\'s a lot of information! Each field is a regular NumPy array, so where do\\nwe get this rich information from? Maybe to your surprise, but the labels of\\nthe DataFrames above were auto-generated from the metadata that\\n[Drawdowns](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)\\nholds. This metadata is called a \"field config\" - a regular\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nthat describes each field (for instance,\\n[Drawdowns.field_config](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns.field_config)).\\nThis makes possible automating and enhancing the behavior of each field. Class\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records),\\nthe base class to all records classes, has many methods to read and interpret\\nthis config.\\n\\n    \\n    \\n    >>> drawdowns.status.values  # (1)!\\n    array([1, 0, 1])\\n    \\n    >>> drawdowns.get_apply_mapping_arr(\\'status\\')  # (2)!\\n    array([\\'Recovered\\', \\'Active\\', \\'Recovered\\'], dtype=object)\\n    \\n\\n  1. Raw data\\n  2. Data enhanced using the field config\\n\\n### Column mapper\\n\\nRecords are one-dimensional structured NumPy arrays. Records from multiple\\ncolumns are concatenated into a single array, so we need a mechanism to group\\nthem by column or group, for instance, to aggregate values column-wise. This\\nis a non-trivial task because finding the records that correspond to a\\nspecific column requires searching all records, which is slow when done\\nrepeatedly. The task of the class\\n[ColumnMapper](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper)\\nis to index all columns only once and cache the result (see\\n[ColumnMapper.col_map](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper.col_map)).\\nA column mapper has at least two more advantages: it allows for grouping\\ncolumns and enables efficient Indexing.\\n\\n    \\n    \\n    >>> drawdowns.col_mapper.col_map  # (1)!\\n    (array([0, 1, 2]), array([2, 1]))\\n    \\n\\n  1. An array with the indices of records ordered by column, and an array with the number of records per column\\n\\nThe produced column map means that the column `a` has two records at indices 0\\nand 1, while the column `b` is represented by only one record at index 2.\\n\\n### Mapped arrays\\n\\nIf\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)\\nis our own DataFrame for events, then\\n[MappedArray](https://vectorbt.pro/pvt_6739a1da/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray)\\nis our own Series! Each field in records can be mapped into a _mapped_ array.\\nIn fact, a mapped array is where most calculations take place. It\\'s just like\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor),\\nbut with a totally different representation of data: one-dimensional and\\nclustered instead of two-dimensional and column-wise. We can even seemingly\\nconvert between both representations. Why wouldn\\'t we then simply convert a\\nmapped array into a regular Series and do all the analysis there? There are\\nseveral reasons:\\n\\n  1. Event data is usually sparse: If 1,000,000 data points produced 50 events, it\\'s much faster to analyze 50 values then having to convert them back and deal with 9,999,950 NaNs.\\n  2. Multiple events, such as orders, can happen within the same bar, which cannot be represented in Pandas efficiently\\n\\nLet\\'s analyze the drawdown values of `drawdowns`:\\n\\n    \\n    \\n    >>> dd_ma = drawdowns.drawdown\\n    >>> dd_ma\\n    <vectorbtpro.records.mapped_array.MappedArray at 0x7ff6d8514f98>\\n    \\n    >>> dd_ma.values  # (1)!\\n    array([-0.08333333, -0.07692308, -0.21428571])\\n    \\n    >>> dd_ma.stats(column=\\'a\\')\\n    Start        2020-01-01 00:00:00\\n    End          2020-01-07 00:00:00\\n    Period           7 days 00:00:00\\n    Count                          2\\n    Mean                   -0.080128\\n    Std                     0.004533\\n    Min                    -0.083333\\n    Median                 -0.080128\\n    Max                    -0.076923\\n    Min Index    2020-01-05 00:00:00\\n    Max Index    2020-01-07 00:00:00\\n    Name: a, dtype: object\\n    \\n    >>> dd_ma.to_pd()  # (2)!\\n                       a         b\\n    2020-01-01       NaN       NaN\\n    2020-01-02       NaN       NaN\\n    2020-01-03       NaN       NaN\\n    2020-01-04       NaN       NaN\\n    2020-01-05 -0.083333       NaN\\n    2020-01-06       NaN       NaN\\n    2020-01-07 -0.076923 -0.214286\\n    \\n\\n  1. Holds three values: two from `a` and one from `b`\\n  2. Can be converted into Pandas, but at what cost?\\n\\nThanks to\\n[ColumnMapper](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper)\\nand\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),\\nwe can select rows and columns from a mapped array the same way as from\\nrecords or any regular Pandas object:\\n\\n    \\n    \\n    >>> dd_ma[\\'b\\'].values\\n    array([-0.21428571])\\n    \\n\\n## Summary\\n\\nKudos for following me all the way down here! The classes that we just covered\\nbuild a strong foundation for data analysis with vectorbt; they implement\\ndesign patterns that are encountered in most other places across the codebase,\\nwhich makes them very easy to recognize and extend. In fact, the most hard-\\ncore class\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)\\nis very similar to our `CorrStats`.\\n\\nYou\\'re now more than ready for using vectorbt, soldier\\n![ðŸŒŸ](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/1f31f.svg)\\n\\n[ Python\\ncode](https://vectorbt.pro/pvt_6739a1da/assets/jupytext/documentation/building-\\nblocks.py.txt)', start_char_idx=44550, end_char_idx=59233, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.18788749762399926),\n",
       " NodeWithScore(node=TextNode(id_='23a68867-577b-45cd-8c81-324775b1b92b', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='90d71dd6-6e62-4f08-93e8-636d0ed92371', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='503923925f0ff1b8594861618671011d019feecef6f8236bd14272ed1166404e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7f856c6e-02ee-4d7d-bc80-c1499f8878dd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d5752d2851ff8275ac2741c9dd5dd167e6f0d8a661f56fc4e723f2174ab0f5be')}, text='...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    \\n\\n  1. We need to specify the argument names that we are about to pass to [Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) (or set `_expected_keys=None` to disable)\\n  2. Convenient class method to broadcast objects, create a wrapper, and pass everything to the constructor\\n  3. Indexing method that uses the wrapper to translate `pd_indexing_func` to an array of selected rows and columns, applies them on both NumPy arrays, and creates a new `CorrStats` instance\\n  4. Computation is done on NumPy objects and the result is converted into Pandas\\n\\nAs you might have noticed, we replaced the superclasses\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured)\\nand\\n[PandasIndexer](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer)\\nwith a single superclass\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping),\\nwhich already inherits them both. Another change applies to the arguments\\ntaken by `CorrStats`: instead of taking two Pandas objects, it now takes\\n`wrapper` of type\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\nalong with the NumPy arrays `obj1` and `obj2`. This has several benefits:\\nwe\\'re keeping Pandas metadata consistent and managed by a single variable,\\nwhile all actions are efficiently performed using NumPy alone. Whenever there\\nis a need to present the findings, we can call\\n[ArrayWrapper.wrap_reduced](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap_reduced)\\nand\\n[ArrayWrapper.wrap](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)\\nto transform them back into a Pandas format, which is done inside the methods\\n`CorrStats.corr` and `CorrStats.rolling_corr` respectively.\\n\\nSince we don\\'t want to force ourselves and the user to create an array wrapper\\nmanually, we also implemented the class method `CorrStats.from_objs`, which\\nbroadcasts both arrays and instantiates `CorrStats`. This way, we can provide\\narray-like objects of any kind and `CorrStats` will automatically build the\\nwrapper for us. Let\\'s illustrate this by computing the correlation coefficient\\nfor `df1` and `df2`, and then for `df1` and parametrized `df2`:\\n\\n    \\n    \\n    >>> df1.corrwith(df2)  # (1)!\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2)\\n    >>> corrstats.corr()  # (2)!\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> df2_sh = vbt.pd_acc.concat(\\n    ...     df2, df2.vbt.shuffle(seed=42), \\n    ...     keys=[\\'plain\\', \\'shuffled\\'])\\n    >>> df2_sh \\n                  plain    shuffled   \\n                   a  b        a  b\\n    2020-01-01     1  5        2  2\\n    2020-01-02     2  4        5  4\\n    2020-01-03     3  3        3  3\\n    2020-01-04     4  2        1  5\\n    2020-01-05     5  1        4  1\\n    \\n    >>> df1.corrwith(df2_sh)  # (3)!\\n    ValueError: cannot join with no overlapping index names\\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2_sh)\\n    >>> corrstats.corr()  # (4)!\\n    plain     a    0.3\\n              b   -0.3\\n    shuffled  a    0.4\\n              b   -0.9\\n    dtype: float64\\n    \\n\\n  1. Using Pandas\\n  2. Using our class\\n  3. Pandas doesn\\'t know how to join this information\\n  4. vectorbt knows exactly what to do thanks to smart broadcasting\\n\\nHere\\'s why we ditched Pandas in favor of Numba:\\n\\n    \\n    \\n    >>> big_df1 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    >>> big_df2 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    \\n    >>> %timeit big_df1.rolling(10).corr(big_df2)  # (1)!\\n    271 ms Â± 13.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\\n    \\n    >>> corrstats = CorrStats.from_objs(big_df1, big_df2)\\n    >>> %timeit corrstats.rolling_corr(10)  # (2)!\\n    12 ms Â± 50.5 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\\n    \\n\\n  1. Using Pandas\\n  2. Using our class\\n\\nAnother addition we made concerns indexing. Since `obj1` and `obj2` are not\\nregular Pandas objects anymore, we cannot simply apply `pd_indexing_func` on\\nthem. Instead, we can use the method\\n[ArrayWrapper.indexing_func_meta](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.indexing_func_meta)\\nto get the rows, columns, and groups that this operation would select. We then\\napply those arrays on both NumPy objects. This approach is exceptionally\\nuseful because now we can select any data from the final shape:\\n\\n    \\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2_sh)\\n    >>> corrstats.loc[\\'2020-01-02\\':\\'2020-01-05\\'].rolling_corr(3)  # (1)!\\n                             plain            shuffled\\n                       a         b         a         b\\n    2020-01-02       NaN       NaN       NaN       NaN\\n    2020-01-03       NaN       NaN       NaN       NaN\\n    2020-01-04 -0.327327  0.327327  0.327327 -0.981981\\n    2020-01-05  0.500000 -0.240192 -0.654654 -0.960769\\n    \\n\\n  1. Compute the rolling correlation coefficient only between 2020-01-02 and 2020-01-05\\n\\nNote\\n\\nNot all classes support indexing on rows. To make sure you can select rows,\\ncheck whether the instance property `column_only_select` is False.\\n\\nThat\\'s how most high-tier classes in vectorbt are built. As a rule of thumb:\\n\\n  * For performance reasons, vectorbt mostly follows the following process: Pandas ![âž¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/27a1.svg) NumPy/Numba ![âž¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/27a1.svg) Pandas. The first stage builds a wrapper from Pandas objects, while the last stage uses the wrapper to present the results to the user.\\n  * Pandas metadata such as shape and index is accessible via the attribute `wrapper`. For example, `wrapper.ndim` returns the number of dimensions the current instance holds.\\n  * Don\\'t call classes directly. Use the class methods, which usually start with the preffix `from_`. The constructor `__init__` is most likely reserved for indexing purposes (by the way, that\\'s why we use `vbt.MA.run()` instead of `vbt.MA()`)\\n  * Wrapper allows indexing of any objects that have elements aligned per row or column, even of more complex layouts (see [Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records))\\n\\n### Base accessor\\n\\nThis subpackage also contains the accessor\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor),\\nwhich exposes many basic operations to the end user and is being subclassed by\\nall other accessors. It inherits\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nand thus we can do with it everything what we can do with our custom\\n`CorrStats` class. Why calling an accessor a \"base\" accessor? Because it is\\nthe superclass of all other accessors in vectorbt and provides them with the\\ncore combining, reshaping, and indexing functions, such as\\n[BaseAccessor.to_2d_array](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.to_2d_array)\\nto convert a Pandas object into a two-dimensional NumPy array.\\n\\nThe \"access\" to the accessor is simple:\\n\\n    \\n    \\n    >>> df.vbt\\n    <vectorbtpro.accessors.Vbt_DFAccessor at 0x7fe3c19f7d68>\\n    \\n    >>> df.vbt.to_2d_array()\\n    array([[ 0,  5, 10, 15],\\n           [ 1,  6, 11, 16],\\n           [ 2,  7, 12, 17],\\n           [ 3,  8, 13, 18],\\n           [ 4,  9, 14, 19]])\\n    \\n\\nIn the example above,\\n[Vbt_DFAccessor](https://vectorbt.pro/pvt_6739a1da/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)\\nis the main accessor for DataFrames, and as you can see in its definition,\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nappears in its superclasses.\\n\\nProbably the most interesting is the method\\n[BaseAccessor.combine](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine),\\nwhich allows for broadcasting and combining the current Pandas object with an\\narbitrary number of other array-like objects given the function `combine_func`\\n(mainly using NumPy).\\n\\n    \\n    \\n    >>> pd.Series([1, 2, 3]).vbt.combine(np.array([[4, 5, 6]]), np.add)\\n       0  1  2\\n    0  5  6  7\\n    1  6  7  8\\n    2  7  8  9\\n    \\n\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nimplements a range of\\n[unary](https://vectorbt.pro/pvt_6739a1da/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.unary_magic_config)\\nand\\n[binary](https://vectorbt.pro/pvt_6739a1da/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.binary_magic_config)\\nmagic methods using this method. For example, let\\'s invoke\\n`BaseAccessor.__add__`, which implements addition:\\n\\n    \\n    \\n    >>> pd.Series([1, 2, 3]) + np.array([[4, 5, 6]])  # (1)!\\n    ValueError: Length of values (1) does not match length of index (3)\\n    \\n    >>> pd.Series([1, 2, 3]).vbt + np.array([[4, 5, 6]])  # (2)!\\n       0  1  2\\n    0  5  6  7\\n    1  6  7  8\\n    2  7  8  9\\n    \\n\\n  1. Without `.vbt`, the addition is done by Pandas\\n  2. With `.vbt`, the addition is done by vectorbt\\n\\nAll of these magic methods were added using class decorators. There are a lot\\nof class decorators for all kind of things in vectorbt. Usually, they take a\\nconfig and attach many attributes at once in some automated way.\\n\\n## Generic\\n\\n    \\n    \\n    flowchart TD;\\n        Analyzable -->|inherits| Wrapping;\\n        Analyzable -->|inherits| StatsBuilderMixin;\\n        Analyzable -->|inherits| PlotsBuilderMixin;\\n        GenericAccessor -->|inherits| BaseAccessor;\\n        GenericAccessor -->|inherits| Analyzable;\\n\\nSubpackage [generic](https://vectorbt.pro/pvt_6739a1da/api/generic/) is the\\ncomputational core of vectorbt. It contains modules for processing and\\nplotting time series and numeric data in general. More importantly: it\\nimplements an [arsenal](https://vectorbt.pro/pvt_6739a1da/api/generic/nb/) of\\nNumba-compiled functions for accelerating and extending Pandas! Those\\nfunctions are powering many corners of vectorbt, from indicators to portfolio\\nanalysis. But for now, let\\'s focus on classes that could make our `CorrStats`\\nclass more powerful.\\n\\n### Builder mixins\\n\\nBuilder [mixins](https://en.wikipedia.org/wiki/Mixin) are classes that, once\\nsubclassed by a class, allow to build a specific functionality from this\\nclass\\' attributes. There are two prominent members:\\n[StatsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)\\nand\\n[PlotsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin).\\nThe former exposes the method\\n[StatsBuilderMixin.stats](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)\\nto compute various metrics, while the latter exposes the method\\n[PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)\\nto display various subplots. Both are subclassed by almost every class that\\ncan analyze data.\\n\\n### Analyzing\\n\\nClass\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)\\ncombines\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nand Builder mixins. It combines everything we introduced above to build a\\nfoundation for a seamless data analysis; that\\'s why it\\'s being subclassed by\\nso many high-tier classes, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)\\nand\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records).\\n\\nWhat are we waiting for? Let\\'s adapt our `CorrStats` class to become\\nanalyzable!\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Analyzable):  # (1)!\\n    ...     _expected_keys = vbt.Analyzable._expected_keys | {\"obj1\", \"obj2\"}\\n    ...\\n    ...     @classmethod\\n    ...     def from_objs(cls, obj1, obj2):\\n    ...         (obj1, obj2), wrapper = vbt.broadcast(\\n    ...             obj1, obj2, \\n    ...             to_pd=False,\\n    ...             return_wrapper=True\\n    ...         )\\n    ...         return cls(wrapper, obj1, obj2)\\n    ...\\n    ...     def __init__(self, wrapper, obj1, obj2):\\n    ...         vbt.Analyzable.__init__(self, wrapper, obj1=obj1, obj2=obj2)  # (2)!\\n    ...\\n    ...         self._obj1 = vbt.to_2d_array(obj1)\\n    ...         self._obj2 = vbt.to_2d_array(obj2)\\n    ...\\n    ...     def indexing_func(self, pd_indexing_func, **kwargs):\\n    ...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)\\n    ...         new_wrapper = wrapper_meta[\"new_wrapper\"]\\n    ...         row_idxs = wrapper_meta[\"row_idxs\"]\\n    ...         col_idxs = wrapper_meta[\"col_idxs\"]\\n    ...         return self.replace(\\n    ...             wrapper=new_wrapper,\\n    ...             obj1=self.obj1[row_idxs, :][:, col_idxs],\\n    ...             obj2=self.obj2[row_idxs, :][:, col_idxs]\\n    ...         )\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):\\n    ...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    ...\\n    ...     _metrics = vbt.HybridConfig(  # (3)!', start_char_idx=30279, end_char_idx=44545, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.16618297310404945),\n",
       " NodeWithScore(node=TextNode(id_='90d71dd6-6e62-4f08-93e8-636d0ed92371', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4e21ccac-4641-46a4-864e-4f722ee20b1d', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='c3d2d070c9f3fabbfed6d10b6614aa794bab09774790e5560167db16ac66ab00'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='23a68867-577b-45cd-8c81-324775b1b92b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8d20447567512dd2b4f8ca5dc82593d81f397438fb529710646c9b3d4351c5b6')}, text='You can imagine it implementing an arbitrary logic for a custom `getattr`\\noperation. It\\'s widely used in\\n[StatsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)\\nand\\n[PlotsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin)\\nto execute metrics and subplots respectively as a chain of commands. In other\\nclasses, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),\\nit\\'s also being used for accessing shortcut properties, caching attribute\\naccess, and more. It works in conjunction with\\n[deep_getattr](https://vectorbt.pro/pvt_6739a1da/api/utils/attr_/#vectorbtpro.utils.attr_.deep_getattr),\\nwhich accesses a chain of attributes provided as a specification.\\n\\nLet\\'s compute the min of the rolling mean solely using Pandas and deep\\nattribute resolution:\\n\\n    \\n    \\n    >>> sr = pd.Series([1, 2, 3, 4, 5])\\n    >>> attr_chain = [(\\'rolling\\', (3,)), \\'mean\\', \\'min\\']\\n    >>> vbt.deep_getattr(sr, attr_chain)\\n    2.0\\n    \\n\\nIf any of the above operations were done on a subclass\\n[AttrResolverMixin](https://vectorbt.pro/pvt_6739a1da/api/base/grouping/#vectorbtpro.utils.attr_.AttrResolverMixin),\\nthey could have been preprocessed and postprocessed easily.\\n\\n### Templating\\n\\nTemplates play an important role in the vectorbt\\'s ecosystem. They allow\\npostponing data resolution to a later point in time when there is more\\ninformation available. There are many different templating classes, such as\\n[Rep](https://vectorbt.pro/pvt_6739a1da/api/utils/template/#vectorbtpro.utils.template.Rep)\\nfor replacing an entire string and\\n[Sub](https://vectorbt.pro/pvt_6739a1da/api/utils/template/#vectorbtpro.utils.template.Sub)\\nfor substituting only parts of it (those beginning with `$`).\\n\\nYou can imagine templates being callbacks that are executed at some point\\nduring the execution, mostly after broadcasting or merging keyword arguments.\\nAlso, there exist functions that offer multiple potential substitution points;\\nin such case, they either attempt to substitute the template multiple times\\nuntil they succeed, or they match the template with a specific substitution id\\n(`sub_id`), if provided. The actual substitution operation is performed by\\n[substitute_templates](https://vectorbt.pro/pvt_6739a1da/api/utils/template/#vectorbtpro.utils.template.substitute_templates).\\n\\n    \\n    \\n    >>> def some_function(*args, **kwargs):\\n    ...     context = {}\\n    ...     args = vbt.substitute_templates(args, context=context, strict=False)\\n    ...     kwargs = vbt.substitute_templates(kwargs, context=context, strict=False)\\n    ...     print(args)\\n    ...     print(kwargs)\\n    ...     \\n    ...     context[\\'result\\'] = 100  # (1)!\\n    ...     args = vbt.substitute_templates(args, context=context)\\n    ...     kwargs = vbt.substitute_templates(kwargs, context=context)\\n    ...     print(args)\\n    ...     print(kwargs)\\n    \\n    >>> some_function(vbt.Rep(\\'result\\'), double_result=vbt.RepEval(\\'result * 2\\'))\\n    (Rep(template=\\'result\\', context=None, strict=None, sub_id=None),)\\n    {\\'double_result\\': RepEval(template=\\'result * 2\\', context=None, strict=None, sub_id=None)}\\n    (100,)\\n    {\\'double_result\\': 200}\\n    \\n\\n  1. New context\\n\\n## Base\\n\\n    \\n    \\n    flowchart TD;\\n        Grouper -->|inherits| Configured;\\n        ArrayWrapper -->|inherits| Configured;\\n        ArrayWrapper -->|inherits| PandasIndexer;\\n        ArrayWrapper -.->|references| Grouper;\\n        Wrapping -->|inherits| Configured;\\n        Wrapping -->|inherits| PandasIndexer;\\n        Wrapping -->|inherits| AttrResolverMixin;\\n        Wrapping -.->|references| ArrayWrapper;\\n        BaseAccessor -->|inherits| Wrapping;\\n\\nSubpackage [base](https://vectorbt.pro/pvt_6739a1da/api/base/) is the non-\\ncomputational core of vectorbt. It offers a range of modules for working with\\nand converting between Pandas and NumPy objects. In particular, it provides\\nfunctions and classes for broadcasting, combining and wrapping NumPy arrays,\\ngrouping columns, managing [MultiIndex](https://pandas.pydata.org/pandas-\\ndocs/stable/user_guide/advanced.html), and more. These operations are\\nessential for extending Pandas and replicating some of its functionality in\\ncustom classes.\\n\\n### Grouping\\n\\nSince vectorbt usually associates with processing multi-column data, where\\neach column (or \"line\") represents a separate backtesting instance, the\\nability to group those columns into some sort of groups is a must-have\\nfeature.\\n\\nClass\\n[Grouper](https://vectorbt.pro/pvt_6739a1da/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper)\\nimplements functionality to validate and build groups of any Pandas Index,\\nespecially columns. It\\'s capable of translating various metadata such as\\n[GroupBy objects](https://pandas.pydata.org/docs/reference/groupby.html) and\\ncolumn levels into special NumPy arrays that can be used by Numba-compiled\\nfunctions to aggregate multiple columns of data. This is especially useful for\\nmulti-asset portfolios where each group is composed of one or more assets.\\n\\n    \\n    \\n    >>> columns = pd.MultiIndex.from_tuples([\\n    ...     (\\'BTC-USD\\', \\'group1\\'), \\n    ...     (\\'ETH-USD\\', \\'group1\\'), \\n    ...     (\\'ADA-USD\\', \\'group2\\'),\\n    ...     (\\'SOL-USD\\', \\'group2\\')\\n    ... ], names=[\\'symbol\\', \\'group\\'])\\n    >>> vbt.Grouper(columns, \\'group\\').get_groups()\\n    array([0, 0, 1, 1])\\n    \\n\\n### Indexing\\n\\nThe main purpose of indexing in vectorbt is to provide Pandas indexing to any\\ncustom class holding Pandas-like objects, in particular, to select rows,\\ncolumns, and groups in each. This is done by forwarding a Pandas indexing\\noperation to each Pandas-like object and instantiating the class using them,\\nwhich is fairly easy using\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured).\\nThis way, one can index complex classes with dozens of Pandas-like objects\\nusing a single command.\\n\\nThe main indexer class\\n[PandasIndexer](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer)\\nmimics a regular Pandas object by exposing properties\\n[PandasIndexer.iloc](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.iloc),\\n[PandasIndexer.loc](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.loc),\\nand\\n[PandasIndexer.xs](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.xs).\\nAll we have to do is to subclass this class and override\\n[IndexingBase.indexing_func](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.IndexingBase.indexing_func),\\nwhich should take `pd_indexing_func`, apply it on each Pandas-like object, and\\ninitialize a new instance.\\n\\nLet\\'s extend our newly created `CorrStats` class with Pandas indexing:\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Configured, vbt.PandasIndexer):\\n    ...     def __init__(self, obj1, obj2):\\n    ...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)\\n    ...         vbt.PandasIndexer.__init__(self)\\n    ...\\n    ...         self._obj1 = obj1\\n    ...         self._obj2 = obj2\\n    ...\\n    ...     def indexing_func(self, pd_indexing_func):  # (1)!\\n    ...         return self.replace(\\n    ...             obj1=pd_indexing_func(self.obj1),\\n    ...             obj2=pd_indexing_func(self.obj2)\\n    ...         )\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):\\n    ...         if isinstance(self.obj1, pd.Series):\\n    ...             return self.obj1.corr(self.obj2)\\n    ...         return self.obj1.corrwith(self.obj2)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         return self.obj1.rolling(window).corr(self.obj2)\\n    \\n    >>> corrstats = CorrStats(df1, df2)\\n    >>> corrstats.corr()\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> corrstats.loc[\\'2020-01-01\\':\\'2020-01-03\\', \\'a\\'].corr()  # (2)!\\n    0.24019223070763066\\n    \\n\\n  1. Here, `pd_indexing_func` is just `lambda x: x.loc[key]`\\n  2. Running the correlation coefficient on a subset of data\\n\\nWe just indexed two Pandas objects as a single entity. Yay!\\n\\n### Wrapping\\n\\nRemember how vectorbt specializes at taking a Pandas object, extracting its\\nNumPy array, processing the array, and converting the results back into a\\nPandas format? The last part is done by the class\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper),\\nwhich captures all the necessary metadata, such as the index, columns, and\\nnumber of dimensions, and exposes methods such as\\n[ArrayWrapper.wrap](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)\\nto convert a NumPy object back into a Pandas format.\\n\\nClass\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\ncombines many concepts we introduced earlier to behave just like a\\n(supercharged) Pandas object. In particular, it uses Grouping to build and\\nmanage groups of columns, and Indexing to select rows, columns, and groups\\nusing Pandas indexing. Probably the most powerful features of an array wrapper\\nis 1) the ability to behave like a grouped object, which isn\\'t possible with\\nPandas alone, and 2) the ability to translate a Pandas indexing operation to a\\nrange of integer arrays that can be used to index NumPy arrays. The latter\\nallows indexing without the need to hold Pandas objects, only the wrapper.\\n\\nWe can construct a wrapper in multiple ways, the easiest being using a Pandas\\nobject:\\n\\n    \\n    \\n    >>> df = pd.DataFrame({\\n    ...     \\'a\\': range(0, 5),\\n    ...     \\'b\\': range(5, 10),\\n    ...     \\'c\\': range(10, 15),\\n    ...     \\'d\\': range(15, 20)\\n    ... }, index=index)\\n    >>> wrapper = vbt.ArrayWrapper.from_obj(df)\\n    >>> print(wrapper)\\n    ArrayWrapper(\\n        index=<DatetimeIndex object at 0x7ff6d8528d68 of shape (5,)>,\\n        columns=<Index object at 0x7ff6d857fcc0 of shape (4,)>,\\n        ndim=2,\\n        freq=None,\\n        column_only_select=None,\\n        group_select=None,\\n        grouped_ndim=None,\\n        grouper=Grouper(\\n            index=<Index object at 0x7ff6d857fcc0 of shape (4,)>,\\n            group_by=None,\\n            allow_enable=True,\\n            allow_disable=True,\\n            allow_modify=True\\n        )\\n    )\\n    \\n\\nLet\\'s create a function that sums all elements over each column using NumPy\\nand returns a regular Pandas object:\\n\\n    \\n    \\n    >>> def sum_per_column(df):\\n    ...     wrapper = vbt.ArrayWrapper.from_obj(df)\\n    ...     result = np.sum(df.values, axis=0)\\n    ...     return wrapper.wrap_reduced(result)\\n    \\n    >>> sum_per_column(df)\\n    a    10\\n    b    35\\n    c    60\\n    d    85\\n    dtype: int64\\n    \\n\\nThe function above is already 20x faster than Pandas\\n![ðŸ¤¯](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/1f92f.svg)\\n\\n    \\n    \\n    >>> big_df = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    \\n    >>> %timeit big_df.sum()\\n    2.52 ms Â± 3.5 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\\n    \\n    >>> %timeit sum_per_column(big_df)\\n    428 Âµs Â± 1.5 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\\n    \\n\\nSince\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\ncan manage groups of columns, let\\'s adapt our function to sum all elements\\nover each group of columns:\\n\\n    \\n    \\n    >>> def sum_per_group(df, group_by):\\n    ...     wrapper = vbt.ArrayWrapper.from_obj(df, group_by=group_by)\\n    ...     results = []\\n    ...     for group_idxs in wrapper.grouper.yield_group_idxs():\\n    ...         group_result = np.sum(df.values[:, group_idxs])\\n    ...         results.append(group_result)\\n    ...     return wrapper.wrap_reduced(results)\\n    \\n    >>> sum_per_group(df, False)  # (1)!\\n    a    10\\n    b    35\\n    c    60\\n    d    85\\n    dtype: int64\\n    \\n    >>> sum_per_group(df, True)  # (2)!\\n    190\\n    \\n    >>> group_by = pd.Index([\\'group1\\', \\'group1\\', \\'group2\\', \\'group2\\'])\\n    >>> sum_per_group(df, group_by)  # (3)!\\n    group1     45\\n    group2    145\\n    dtype: int64\\n    \\n\\n  1. No grouping (one group per column)\\n  2. One group with all columns\\n  3. Multiple groups with multiple columns\\n\\nTo avoid creating multiple array wrappers with the same metadata, there is the\\nclass\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping),\\nwhich binds a single instance of\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\nto manage an arbitrary number of shape-compatible array-like objects. Instead\\nof accepting multiple Pandas objects, it takes an array wrapper, and all other\\nobjects and arrays in any format (preferably NumPy), and wraps them using this\\nwrapper. Additionally, any\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nsubclass can utilize its wrapper to perform Pandas indexing on any kind of\\nobjects, including NumPy arrays, because\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\ncan translate a Pandas indexing operation into universal row, column, and\\ngroup indices.\\n\\nComing back to our `CorrStats` class. There are two issues with the current\\nimplementation:\\n\\n  1. Both metrics require Pandas objects of identical layouts as inputs\\n  2. Both metrics are computed using Pandas, which is rather slow\\n\\nLet\\'s upgrade our `CorrStats` class to work on NumPy arrays and with an array\\nwrapper:\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Wrapping):\\n    ...     _expected_keys = vbt.Wrapping._expected_keys | {\"obj1\", \"obj2\"}  # (1)!\\n    ...\\n    ...     @classmethod\\n    ...     def from_objs(cls, obj1, obj2):  # (2)!\\n    ...         (obj1, obj2), wrapper = vbt.broadcast(\\n    ...             obj1, obj2, \\n    ...             to_pd=False,\\n    ...             return_wrapper=True\\n    ...         )\\n    ...         return cls(wrapper, obj1, obj2)\\n    ...\\n    ...     def __init__(self, wrapper, obj1, obj2):\\n    ...         vbt.Wrapping.__init__(self, wrapper, obj1=obj1, obj2=obj2)\\n    ...\\n    ...         self._obj1 = vbt.to_2d_array(obj1)\\n    ...         self._obj2 = vbt.to_2d_array(obj2)\\n    ...\\n    ...     def indexing_func(self, pd_indexing_func, **kwargs):  # (3)!\\n    ...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)\\n    ...         new_wrapper = wrapper_meta[\"new_wrapper\"]\\n    ...         row_idxs = wrapper_meta[\"row_idxs\"]\\n    ...         col_idxs = wrapper_meta[\"col_idxs\"]\\n    ...         return self.replace(\\n    ...             wrapper=new_wrapper,\\n    ...             obj1=self.obj1[row_idxs, :][:, col_idxs],\\n    ...             obj2=self.obj2[row_idxs, :][:, col_idxs]\\n    ...         )\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):  # (4)!\\n    ...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    \\n\\n  1.', start_char_idx=15049, end_char_idx=30629, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.14655897562983655),\n",
       " NodeWithScore(node=TextNode(id_='4e21ccac-4641-46a4-864e-4f722ee20b1d', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='68d0cdf5-4284-42ee-ad0d-44e531f2e967', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/cookbook-persistence.txt', 'file_name': 'cookbook-persistence.txt', 'file_type': 'text/plain', 'file_size': 3968, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='725fb4e117de05f69ff888f9a426d93861552dcd079222d8651f48cc8d958872'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='90d71dd6-6e62-4f08-93e8-636d0ed92371', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8fd4f68faabe3461f36b7a90c97b320901df4d494c3b2ad9d96b5fd958f150e6')}, text='#  Building blocks\\n\\nIn what follows, we will look at sub-packages, modules, and especially classes\\nthat act as building blocks for more advanced functionalities distributed\\nacross vectorbt, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).\\nFor illustration, we will gradually build our custom class `CorrStats` that\\nwill let us analyze the correlation between two arrays in the most performant\\nand flexible way\\n![ðŸ§ ](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/1f9e0.svg)\\n\\n## Utilities\\n\\n    \\n    \\n    flowchart TD;\\n        Config -->|inherits| Pickleable;\\n        Config -->|inherits| Prettified;\\n        Configured -->|inherits| Pickleable;\\n        Configured -->|inherits| Prettified;\\n        Configured -.->|references| Config;\\n        AttrResolverMixin;\\n\\n(Reload the page if the diagram doesn\\'t show up)\\n\\nVectorBT PRO deploys a modular project structure that is composed of a range\\nof subpackages. Each subpackage is applicable to a certain area of analysis.\\n\\nSubpackage [utils](https://vectorbt.pro/pvt_6739a1da/api/utils/) contains a\\nset of utilities powering every part of vectorbt\\n![âš¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/26a1.svg)\\nThey are loosely connected and provide small but powerful re-usable code\\nsnippets that can be used independently of other functionality.\\n\\nInfo\\n\\nThe main reason why we don\\'t import third-party packages but implement many\\nutilities from scratch is because we want to retain full control over\\nexecution and code quality.\\n\\n### Formatting\\n\\nVectorBT PRO implements its own formatting engine that can pretty-print any\\nPython object. It\\'s far more superior to formatting with\\n[JSON](https://en.wikipedia.org/wiki/JSON) because it respects native Python\\ndata types and injects some smart formatting logic when it comes to more\\nstructured data types, such as `np.dtype` and `namedtuple`. Often, we can even\\nconvert the string back into Python using `eval`.\\n\\nLet\\'s beautify a nested dictionary using\\n[prettify](https://vectorbt.pro/pvt_6739a1da/api/utils/formatting/#vectorbtpro.utils.formatting.prettify)\\nand convert the string back into an object:\\n\\n    \\n    \\n    >>> import vectorbtpro as vbt\\n    >>> import numpy as np\\n    >>> import pandas as pd\\n    >>> from datetime import datetime, timedelta\\n    \\n    >>> dct = {\\'planet\\' : {\\'has\\': {\\'plants\\': \\'yes\\', \\'animals\\': \\'yes\\', \\'cryptonite\\': \\'no\\'}, \\'name\\': \\'Earth\\'}}\\n    >>> print(vbt.prettify(dct))\\n    {\\n        \\'planet\\': {\\n            \\'has\\': {\\n                \\'plants\\': \\'yes\\',\\n                \\'animals\\': \\'yes\\',\\n                \\'cryptonite\\': \\'no\\'\\n            },\\n            \\'name\\': \\'Earth\\'\\n        }\\n    }\\n    \\n    >>> eval(vbt.prettify(dct)) == dct\\n    True\\n    \\n\\nHint\\n\\nWondering why we used `vbt.prettify` instead of\\n`vbt.utils.formatting.prettify`? Any utility that may prove useful to the end\\nuser can be accessed directly from `vbt`.\\n\\nTo see which utilities are accessible from the root of the package, visit\\n[vectorbtpro/utils/__init__.py](https://github.com/polakowo/vectorbt.pro/blob/main/vectorbtpro/utils/__init__.py)\\nor any other subpackage, and look for the objects that are listed in\\n`__all__`.\\n\\nClass\\n[Prettified](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.formatting.Prettified)\\nimplements the abstract method\\n[Prettified.prettify](https://vectorbt.pro/pvt_6739a1da/api/utils/formatting/#vectorbtpro.utils.formatting.Prettified.prettify),\\nwhich a subclass can override to pretty-print an instance using\\n[prettify](https://vectorbt.pro/pvt_6739a1da/api/utils/formatting/#vectorbtpro.utils.formatting.prettify).\\nRead below to learn how instances of various classes can be introspected using\\nthis method.\\n\\n### Pickling\\n\\nPickling is the process of converting a Python object into a byte stream to\\nstore it in a file/database. Class\\n[Pickleable](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable)\\nenables pickling of objects of any complexity using\\n[Dill](https://dill.readthedocs.io/en/latest/) (or\\n[pickle](https://docs.python.org/3/library/pickle.html) if Dill is not\\ninstalled). Each of its subclasses inherits ready-to-use methods for\\nserializing, de-serializing, saving to a file, and loading from a file. This\\nis truly amazing because it allows us to persist objects holding any type of\\ndata, including instances of\\n[Data](https://vectorbt.pro/pvt_6739a1da/api/data/base/#vectorbtpro.data.base.Data)\\nand\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).\\n\\n### Configuring\\n\\nVectorBT PRO heavily relies upon automation based on some sort of\\nspecification. The specification for most repetitive tasks is usually stored\\ninside so-called \"configs\", which act as settings for a certain task, a data\\nstructure, or even a class. This makes most places in vectorbt transparent and\\neasily traversable and changeable programmatically.\\n\\nClass\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nis a dictionary on steroids: it extends the Python\\'s `dict` with various\\nconfiguration features, such as frozen keys, read-only values, accessing keys\\nvia the dot notation, and nested updates. The most notable feature is the\\nability to reset a config to its initial state and even make checkpoints,\\nwhich is particularly useful for settings. In addition, since\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\ninherits\\n[Pickleable](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable),\\nwe can save any configuration to disk, while subclassing\\n[Prettified](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.formatting.Prettified)\\nallows us to beautify it (by the way, this approach is being used to generate\\nthe API reference):\\n\\n    \\n    \\n    >>> print(vbt.Records.field_config)\\n    Config(\\n        dtype=None,\\n        settings={\\n            \\'id\\': {\\n                \\'name\\': \\'id\\',\\n                \\'title\\': \\'Id\\'\\n            },\\n            \\'col\\': {\\n                \\'name\\': \\'col\\',\\n                \\'title\\': \\'Column\\',\\n                \\'mapping\\': \\'columns\\'\\n            },\\n            \\'idx\\': {\\n                \\'name\\': \\'idx\\',\\n                \\'title\\': \\'Timestamp\\',\\n                \\'mapping\\': \\'index\\'\\n            }\\n        }\\n    )\\n    \\n\\nConfigs are very common structures in vectorbt. There are three main types of\\nconfigs (that either subclass or partially call\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config))\\nused throughout vectorbt:\\n\\n  1. [ReadonlyConfig](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.ReadonlyConfig) for configurations that aren\\'t meant to be modified, such as [nb_config](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.nb_config), which is being used to attach a number of Numba-compiled functions to [GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor) only once upon importing vectorbt, so if it were modifiable it wouldn\\'t have any effect anyway.\\n  2. [HybridConfig](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.HybridConfig) for configuration that are meant to be modified. The best examples are [Portfolio.metrics](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.metrics) and [Portfolio.subplots](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.subplots), which contain all the metrics and subplots supported by [Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio); they can be easily changed and extended, and then reset if you did some breaking changes.\\n  3. [SettingsConfig](https://vectorbt.pro/pvt_6739a1da/api/_settings/#vectorbtpro._settings.SettingsConfig) holding the global settings defined in [_settings](https://vectorbt.pro/pvt_6739a1da/api/_settings/). It\\'s a custom subclass of [Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config) that is capable of changing Plotly themes and converting any sub-configs of type `dict` to smaller settings accessible via the dot notation (`vbt.settings.portfolio.log` instead of `vbt.settings[\\'portfolio\\'][\\'log\\']`)\\n\\nA config can be created like a regular `dict`. To provide options for this\\nconfig, use the `options_` argument (notice the trailing underscore):\\n\\n    \\n    \\n    >>> cfg = vbt.Config(\\n    ...     hello=\"world\",\\n    ...     options_=dict(readonly=True)\\n    ... )\\n    >>> print(cfg)\\n    Config(\\n        hello=\\'world\\'\\n    )\\n    \\n    >>> cfg[\"change\"] = \"something\"\\n    TypeError: Config is read-only\\n    \\n\\nApart from the use cases listed above,\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nis also being used by the class\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured),\\nwhich is a base class to most core classes in vectorbt. It\\'s a read-only class\\nthat holds a config of type\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nwith all arguments passed during the initialization. Whenever we initialize\\nany subclass of\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured),\\nany named arguments we passed to the initializer (`__init__`) are stored\\ninside\\n[Configured.config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured.config).\\nThis way, the created instance is described and managed solely by its config:\\n\\n  * We can copy/modify the config and pass it to the class to initialize another instance (`new_instance = ConfiguredClass(**old_instance.config)`)\\n  * We can pickle the class and the config to pickle the entire instance\\n  * We can compare instances based on their classes and configs\\n\\nThe main requirement for all of this to work properly is **immutability**. And\\nhere we have arrived at the first very important design decision: most classes\\nin vectorbt are meant to be immutable (read-only) and it\\'s discouraged to\\nchange any attribute unless it\\'s listed in a special variable called\\n`_writeable_attrs`. There are multiple reasons why we require immutability:\\n\\n  1. Immutable instances can be easily recreated and manipulated using their configs\\n  2. Immutable instances are side effect free and can have cached attributes\\n  3. Immutable instances can be hashed\\n\\nLet\\'s create our custom class that returns some correlation statistics of two\\narrays. In particular, it will compute the Pearson correlation coefficient and\\nits rolling version using Pandas:\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Configured):\\n    ...     def __init__(self, obj1, obj2):\\n    ...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)\\n    ...\\n    ...         self._obj1 = obj1\\n    ...         self._obj2 = obj2\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):\\n    ...         if isinstance(self.obj1, pd.Series):\\n    ...             return self.obj1.corr(self.obj2)\\n    ...         return self.obj1.corrwith(self.obj2)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         return self.obj1.rolling(window).corr(self.obj2)\\n    \\n\\nThis is how most configured classes in vectorbt, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),\\nare designed. Any argument that is being passed to `CorrStats` is forwarded\\ndown to\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured)\\nto initialize a new config:\\n\\n    \\n    \\n    >>> index = [datetime(2020, 1, 1) + timedelta(days=i) for i in range(5)]\\n    >>> df1 = pd.DataFrame({\\n    ...     \\'a\\': [1, 5, 2, 4, 3],\\n    ...     \\'b\\': [3, 2, 4, 1, 5]\\n    ... }, index=index)\\n    >>> df2 = pd.DataFrame({\\n    ...     \\'a\\': [1, 2, 3, 4, 5],\\n    ...     \\'b\\': [5, 4, 3, 2, 1]\\n    ... }, index=index)\\n    \\n    >>> corrstats = CorrStats(df1, df2)\\n    >>> print(corrstats.config)\\n    Config(\\n        obj1=<DataFrame object at 0x7fd490461400 of shape (5, 2)>,\\n        obj2=<DataFrame object at 0x7fd490461240 of shape (5, 2)>\\n    )\\n    \\n\\nAccess to any attribute is read-only: whenever we try to set a read-only\\nproperty or modify the config, an (expected) error will be thrown:\\n\\n    \\n    \\n    >>> df3 = pd.DataFrame({\\n    ...     \\'a\\': [3, 2, 1, 5, 4],\\n    ...     \\'b\\': [4, 5, 1, 2, 3]\\n    ... }, index=index)\\n    >>> corrstats.obj1 = df3\\n    AttributeError: can\\'t set attribute\\n    \\n    >>> corrstats.config[\\'obj1\\'] = df3\\n    TypeError: Config is read-only\\n    \\n\\nHowever, it won\\'t (and can\\'t) throw an error when setting a private attribute\\n(with a leading underscore) or if any of the attributes were modified in\\nplace, which is a common pitfall you should avoid.\\n\\n    \\n    \\n    >>> corrstats._obj1 = df3  # (1)!\\n    >>> corrstats.obj1.iloc[:] = df3  # (2)!\\n    \\n\\n  1. This would work, but at what cost?\\n  2. This would work too, unfortunately\\n\\nWarning\\n\\nvectorbt assumes that the data in a configured instance always stays the same.\\nWhenever there is a change to data, vectorbt won\\'t register it and likely\\ndeliver erroneous results at some point in time.\\n\\nThe only approved way to change any data in an instance is to create another\\ninstance!\\n\\nTo change anything, pass the new data to\\n[Configured.replace](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured.replace),\\nwhich takes the same arguments as the class but in the keyword-only format,\\nmerges them over the old config, and passes as keyword arguments to the class\\nfor instantiation.\\n\\n    \\n    \\n    >>> new_corrstats = corrstats.replace(obj1=df3)\\n    >>> new_corrstats.obj1  # (1)!\\n                a  b\\n    2020-01-01  3  4\\n    2020-01-02  2  5\\n    2020-01-03  1  1\\n    2020-01-04  5  2\\n    2020-01-05  4  3\\n    \\n    >>> new_corrstats.obj2  # (2)!\\n                a  b\\n    2020-01-01  1  5\\n    2020-01-02  2  4\\n    2020-01-03  3  3\\n    2020-01-04  4  2\\n    2020-01-05  5  1\\n    \\n\\n  1. `df1` has been replaced with `df3`\\n  2. `df2` remains untouched\\n\\nSince all of our data is now stored inside a config, we can perform many\\nactions on the instance as if we performed them on the config itself, such as\\nsaving to disk (thanks to Pickling):\\n\\n    \\n    \\n    >>> corrstats.save(\\'corrstats\\')  # (1)!\\n    \\n    >>> corrstats = CorrStats.load(\\'corrstats\\')\\n    \\n\\n  1. Saves the instance along with both objects\\n\\n### Attribute resolution\\n\\nAttribute resolution is handy when it comes to accessing attributes based on\\nstrings or some other logic, which is realized by the mixin\\n[AttrResolverMixin](https://vectorbt.pro/pvt_6739a1da/api/base/grouping/#vectorbtpro.utils.attr_.AttrResolverMixin).\\nYou can imagine it implementing an arbitrary logic for a custom `getattr`\\noperation. It\\'s widely used in\\n[StatsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)\\nand\\n[PlotsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin)\\nto execute metrics and subplots respectively as a chain of commands. In other\\nclasses, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),\\nit\\'s also being used for accessing shortcut properties, caching attribute\\naccess, and more.', start_char_idx=0, end_char_idx=15717, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.13463889249429073)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.set_index_id(\"default\")\n",
    "index.storage_context.persist(persist_dir=\"../index_notebook/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "\n",
    "storage_context2 = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"../index_notebook/\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(\n",
    "        persist_dir=\"../index_notebook/\", namespace=\"default\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"../index_notebook/\"),\n",
    ")\n",
    "\n",
    "# index2 = VectorStoreIndex.from_vector_store(vector_store2, service_context=service_context)\n",
    "index2 = load_index_from_storage(storage_context=storage_context2, index_id=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever2 = VectorIndexRetriever(\n",
    "\tindex=index2,\n",
    "\tsimilarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer2 = get_response_synthesizer(\n",
    "\tresponse_mode=\"tree_summarize\", service_context=service_context)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine2 = RetrieverQueryEngine(\n",
    "    retriever=retriever2,\n",
    "    response_synthesizer=response_synthesizer2,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To move the stop loss to breakeven after the first take profit is hit in your strategy, you can follow these steps:\n",
      "\n",
      "1. **Define Your Entry Price and Stop Loss**: Initially, when you enter a trade, you define an entry price and a stop loss. The entry price is the price at which you enter the trade, and the stop loss is the price at which you exit the trade if it moves against you to limit your losses.\n",
      "\n",
      "2. **Set Take Profit Levels**: Since you have 4 take profit levels, you need to define these levels. Each take profit level is a price at which a portion of your trade will be closed in profit. The first take profit level is the one after hitting which you want to move your stop loss to breakeven.\n",
      "\n",
      "3. **Calculate Breakeven Point**: The breakeven point is your entry price. Moving the stop loss to breakeven means adjusting the stop loss to the entry price so that if the price reverses after hitting the first take profit, you exit the trade without a loss.\n",
      "\n",
      "4. **Implement Logic to Adjust Stop Loss**: You need to implement logic in your strategy that continuously monitors the price. Once the price reaches the first take profit level, the logic should automatically adjust the stop loss to the breakeven point (entry price). This ensures that if the price reverses after taking partial profit, you will not incur a loss on the remaining position.\n",
      "\n",
      "5. **Monitor and Execute**: Continuously monitor the trade. If the price hits the first take profit, execute the logic to move the stop loss to the entry price. If the price continues to move in your favor, you can take further profits at the other take profit levels or adjust the stop loss further to lock in profits.\n",
      "\n",
      "6. **Considerations**: When implementing this strategy, consider transaction costs and slippage, as they can affect the breakeven point. Additionally, ensure that your trading platform or programming environment supports conditional orders or has the capability to programmatically adjust stop losses based on price action.\n",
      "\n",
      "This approach allows you to secure your position by ensuring that once the first take profit is hit, the worst-case scenario for the remaining position is breaking even, excluding transaction costs.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"\"\"\n",
    "In my strategy I have 4 take profits and stop loss. How can I move stop loss to breakeven after the first take profit is hit?\n",
    "\"\"\"\n",
    "\n",
    "response = query_engine2.query(test_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='23a68867-577b-45cd-8c81-324775b1b92b', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='90d71dd6-6e62-4f08-93e8-636d0ed92371', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='503923925f0ff1b8594861618671011d019feecef6f8236bd14272ed1166404e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7f856c6e-02ee-4d7d-bc80-c1499f8878dd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d5752d2851ff8275ac2741c9dd5dd167e6f0d8a661f56fc4e723f2174ab0f5be')}, text='...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    \\n\\n  1. We need to specify the argument names that we are about to pass to [Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) (or set `_expected_keys=None` to disable)\\n  2. Convenient class method to broadcast objects, create a wrapper, and pass everything to the constructor\\n  3. Indexing method that uses the wrapper to translate `pd_indexing_func` to an array of selected rows and columns, applies them on both NumPy arrays, and creates a new `CorrStats` instance\\n  4. Computation is done on NumPy objects and the result is converted into Pandas\\n\\nAs you might have noticed, we replaced the superclasses\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured)\\nand\\n[PandasIndexer](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer)\\nwith a single superclass\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping),\\nwhich already inherits them both. Another change applies to the arguments\\ntaken by `CorrStats`: instead of taking two Pandas objects, it now takes\\n`wrapper` of type\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\nalong with the NumPy arrays `obj1` and `obj2`. This has several benefits:\\nwe\\'re keeping Pandas metadata consistent and managed by a single variable,\\nwhile all actions are efficiently performed using NumPy alone. Whenever there\\nis a need to present the findings, we can call\\n[ArrayWrapper.wrap_reduced](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap_reduced)\\nand\\n[ArrayWrapper.wrap](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)\\nto transform them back into a Pandas format, which is done inside the methods\\n`CorrStats.corr` and `CorrStats.rolling_corr` respectively.\\n\\nSince we don\\'t want to force ourselves and the user to create an array wrapper\\nmanually, we also implemented the class method `CorrStats.from_objs`, which\\nbroadcasts both arrays and instantiates `CorrStats`. This way, we can provide\\narray-like objects of any kind and `CorrStats` will automatically build the\\nwrapper for us. Let\\'s illustrate this by computing the correlation coefficient\\nfor `df1` and `df2`, and then for `df1` and parametrized `df2`:\\n\\n    \\n    \\n    >>> df1.corrwith(df2)  # (1)!\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2)\\n    >>> corrstats.corr()  # (2)!\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> df2_sh = vbt.pd_acc.concat(\\n    ...     df2, df2.vbt.shuffle(seed=42), \\n    ...     keys=[\\'plain\\', \\'shuffled\\'])\\n    >>> df2_sh \\n                  plain    shuffled   \\n                   a  b        a  b\\n    2020-01-01     1  5        2  2\\n    2020-01-02     2  4        5  4\\n    2020-01-03     3  3        3  3\\n    2020-01-04     4  2        1  5\\n    2020-01-05     5  1        4  1\\n    \\n    >>> df1.corrwith(df2_sh)  # (3)!\\n    ValueError: cannot join with no overlapping index names\\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2_sh)\\n    >>> corrstats.corr()  # (4)!\\n    plain     a    0.3\\n              b   -0.3\\n    shuffled  a    0.4\\n              b   -0.9\\n    dtype: float64\\n    \\n\\n  1. Using Pandas\\n  2. Using our class\\n  3. Pandas doesn\\'t know how to join this information\\n  4. vectorbt knows exactly what to do thanks to smart broadcasting\\n\\nHere\\'s why we ditched Pandas in favor of Numba:\\n\\n    \\n    \\n    >>> big_df1 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    >>> big_df2 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    \\n    >>> %timeit big_df1.rolling(10).corr(big_df2)  # (1)!\\n    271 ms Â± 13.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\\n    \\n    >>> corrstats = CorrStats.from_objs(big_df1, big_df2)\\n    >>> %timeit corrstats.rolling_corr(10)  # (2)!\\n    12 ms Â± 50.5 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\\n    \\n\\n  1. Using Pandas\\n  2. Using our class\\n\\nAnother addition we made concerns indexing. Since `obj1` and `obj2` are not\\nregular Pandas objects anymore, we cannot simply apply `pd_indexing_func` on\\nthem. Instead, we can use the method\\n[ArrayWrapper.indexing_func_meta](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.indexing_func_meta)\\nto get the rows, columns, and groups that this operation would select. We then\\napply those arrays on both NumPy objects. This approach is exceptionally\\nuseful because now we can select any data from the final shape:\\n\\n    \\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2_sh)\\n    >>> corrstats.loc[\\'2020-01-02\\':\\'2020-01-05\\'].rolling_corr(3)  # (1)!\\n                             plain            shuffled\\n                       a         b         a         b\\n    2020-01-02       NaN       NaN       NaN       NaN\\n    2020-01-03       NaN       NaN       NaN       NaN\\n    2020-01-04 -0.327327  0.327327  0.327327 -0.981981\\n    2020-01-05  0.500000 -0.240192 -0.654654 -0.960769\\n    \\n\\n  1. Compute the rolling correlation coefficient only between 2020-01-02 and 2020-01-05\\n\\nNote\\n\\nNot all classes support indexing on rows. To make sure you can select rows,\\ncheck whether the instance property `column_only_select` is False.\\n\\nThat\\'s how most high-tier classes in vectorbt are built. As a rule of thumb:\\n\\n  * For performance reasons, vectorbt mostly follows the following process: Pandas ![âž¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/27a1.svg) NumPy/Numba ![âž¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/27a1.svg) Pandas. The first stage builds a wrapper from Pandas objects, while the last stage uses the wrapper to present the results to the user.\\n  * Pandas metadata such as shape and index is accessible via the attribute `wrapper`. For example, `wrapper.ndim` returns the number of dimensions the current instance holds.\\n  * Don\\'t call classes directly. Use the class methods, which usually start with the preffix `from_`. The constructor `__init__` is most likely reserved for indexing purposes (by the way, that\\'s why we use `vbt.MA.run()` instead of `vbt.MA()`)\\n  * Wrapper allows indexing of any objects that have elements aligned per row or column, even of more complex layouts (see [Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records))\\n\\n### Base accessor\\n\\nThis subpackage also contains the accessor\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor),\\nwhich exposes many basic operations to the end user and is being subclassed by\\nall other accessors. It inherits\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nand thus we can do with it everything what we can do with our custom\\n`CorrStats` class. Why calling an accessor a \"base\" accessor? Because it is\\nthe superclass of all other accessors in vectorbt and provides them with the\\ncore combining, reshaping, and indexing functions, such as\\n[BaseAccessor.to_2d_array](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.to_2d_array)\\nto convert a Pandas object into a two-dimensional NumPy array.\\n\\nThe \"access\" to the accessor is simple:\\n\\n    \\n    \\n    >>> df.vbt\\n    <vectorbtpro.accessors.Vbt_DFAccessor at 0x7fe3c19f7d68>\\n    \\n    >>> df.vbt.to_2d_array()\\n    array([[ 0,  5, 10, 15],\\n           [ 1,  6, 11, 16],\\n           [ 2,  7, 12, 17],\\n           [ 3,  8, 13, 18],\\n           [ 4,  9, 14, 19]])\\n    \\n\\nIn the example above,\\n[Vbt_DFAccessor](https://vectorbt.pro/pvt_6739a1da/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)\\nis the main accessor for DataFrames, and as you can see in its definition,\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nappears in its superclasses.\\n\\nProbably the most interesting is the method\\n[BaseAccessor.combine](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine),\\nwhich allows for broadcasting and combining the current Pandas object with an\\narbitrary number of other array-like objects given the function `combine_func`\\n(mainly using NumPy).\\n\\n    \\n    \\n    >>> pd.Series([1, 2, 3]).vbt.combine(np.array([[4, 5, 6]]), np.add)\\n       0  1  2\\n    0  5  6  7\\n    1  6  7  8\\n    2  7  8  9\\n    \\n\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nimplements a range of\\n[unary](https://vectorbt.pro/pvt_6739a1da/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.unary_magic_config)\\nand\\n[binary](https://vectorbt.pro/pvt_6739a1da/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.binary_magic_config)\\nmagic methods using this method. For example, let\\'s invoke\\n`BaseAccessor.__add__`, which implements addition:\\n\\n    \\n    \\n    >>> pd.Series([1, 2, 3]) + np.array([[4, 5, 6]])  # (1)!\\n    ValueError: Length of values (1) does not match length of index (3)\\n    \\n    >>> pd.Series([1, 2, 3]).vbt + np.array([[4, 5, 6]])  # (2)!\\n       0  1  2\\n    0  5  6  7\\n    1  6  7  8\\n    2  7  8  9\\n    \\n\\n  1. Without `.vbt`, the addition is done by Pandas\\n  2. With `.vbt`, the addition is done by vectorbt\\n\\nAll of these magic methods were added using class decorators. There are a lot\\nof class decorators for all kind of things in vectorbt. Usually, they take a\\nconfig and attach many attributes at once in some automated way.\\n\\n## Generic\\n\\n    \\n    \\n    flowchart TD;\\n        Analyzable -->|inherits| Wrapping;\\n        Analyzable -->|inherits| StatsBuilderMixin;\\n        Analyzable -->|inherits| PlotsBuilderMixin;\\n        GenericAccessor -->|inherits| BaseAccessor;\\n        GenericAccessor -->|inherits| Analyzable;\\n\\nSubpackage [generic](https://vectorbt.pro/pvt_6739a1da/api/generic/) is the\\ncomputational core of vectorbt. It contains modules for processing and\\nplotting time series and numeric data in general. More importantly: it\\nimplements an [arsenal](https://vectorbt.pro/pvt_6739a1da/api/generic/nb/) of\\nNumba-compiled functions for accelerating and extending Pandas! Those\\nfunctions are powering many corners of vectorbt, from indicators to portfolio\\nanalysis. But for now, let\\'s focus on classes that could make our `CorrStats`\\nclass more powerful.\\n\\n### Builder mixins\\n\\nBuilder [mixins](https://en.wikipedia.org/wiki/Mixin) are classes that, once\\nsubclassed by a class, allow to build a specific functionality from this\\nclass\\' attributes. There are two prominent members:\\n[StatsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)\\nand\\n[PlotsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin).\\nThe former exposes the method\\n[StatsBuilderMixin.stats](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)\\nto compute various metrics, while the latter exposes the method\\n[PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)\\nto display various subplots. Both are subclassed by almost every class that\\ncan analyze data.\\n\\n### Analyzing\\n\\nClass\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)\\ncombines\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nand Builder mixins. It combines everything we introduced above to build a\\nfoundation for a seamless data analysis; that\\'s why it\\'s being subclassed by\\nso many high-tier classes, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)\\nand\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records).\\n\\nWhat are we waiting for? Let\\'s adapt our `CorrStats` class to become\\nanalyzable!\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Analyzable):  # (1)!\\n    ...     _expected_keys = vbt.Analyzable._expected_keys | {\"obj1\", \"obj2\"}\\n    ...\\n    ...     @classmethod\\n    ...     def from_objs(cls, obj1, obj2):\\n    ...         (obj1, obj2), wrapper = vbt.broadcast(\\n    ...             obj1, obj2, \\n    ...             to_pd=False,\\n    ...             return_wrapper=True\\n    ...         )\\n    ...         return cls(wrapper, obj1, obj2)\\n    ...\\n    ...     def __init__(self, wrapper, obj1, obj2):\\n    ...         vbt.Analyzable.__init__(self, wrapper, obj1=obj1, obj2=obj2)  # (2)!\\n    ...\\n    ...         self._obj1 = vbt.to_2d_array(obj1)\\n    ...         self._obj2 = vbt.to_2d_array(obj2)\\n    ...\\n    ...     def indexing_func(self, pd_indexing_func, **kwargs):\\n    ...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)\\n    ...         new_wrapper = wrapper_meta[\"new_wrapper\"]\\n    ...         row_idxs = wrapper_meta[\"row_idxs\"]\\n    ...         col_idxs = wrapper_meta[\"col_idxs\"]\\n    ...         return self.replace(\\n    ...             wrapper=new_wrapper,\\n    ...             obj1=self.obj1[row_idxs, :][:, col_idxs],\\n    ...             obj2=self.obj2[row_idxs, :][:, col_idxs]\\n    ...         )\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):\\n    ...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    ...\\n    ...     _metrics = vbt.HybridConfig(  # (3)!', start_char_idx=30279, end_char_idx=44545, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.027993025644548548),\n",
       " NodeWithScore(node=TextNode(id_='7f856c6e-02ee-4d7d-bc80-c1499f8878dd', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='23a68867-577b-45cd-8c81-324775b1b92b', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='4a572a8f360d1d09082cb7e16863d79c1c943beb21ed4e3c034b62abc52b9ae5')}, text='...         corr=dict(\\n    ...             title=\\'Corr. Coefficient\\',\\n    ...             calc_func=\\'corr\\'\\n    ...         )\\n    ...     )\\n    ...\\n    ...     _subplots = vbt.HybridConfig(  # (4)!\\n    ...          rolling_corr=dict(\\n    ...              title=vbt.Sub(\"Rolling Corr. Coefficient (window=$window)\"),\\n    ...              plot_func=vbt.Sub(\\'rolling_corr($window).vbt.plot\\'),  # (5)!\\n    ...              pass_trace_names=False\\n    ...          )\\n    ...     )\\n    \\n\\n  1. Replace `Wrapping` with `Analyzable`\\n  2. Replace `Wrapping` with `Analyzable`\\n  3. Define default metrics for [StatsBuilderMixin.stats](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)\\n  4. Define default subplots for [PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)\\n  5. Recall Templating and Attribute resolution?\\n\\nWe changed a few things: replaced `Wrapping` with `Analyzable`, and added some\\nmetrics and subplots based on `CorrStats.corr` and `CorrStats.rolling_corr`.\\nThat\\'s it! We can now pass arbitrary array-like objects to\\n`CorrStats.from_objs` and it will return an instance that can be used to\\nanalyze the correlation between the objects, in particular using\\n`CorrStats.stats` and `CorrStats.plots`:\\n\\n    \\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2)\\n    >>> corrstats.stats(column=\\'a\\')  # (1)!\\n    Corr. Coefficient    0.3\\n    Name: a, dtype: object\\n    \\n    >>> corrstats[\\'a\\'].stats()  # (2)!\\n    Corr. Coefficient    0.3\\n    Name: a, dtype: object\\n    \\n    >>> corrstats.plots(template_context=dict(window=3)).show()  # (3)!\\n    \\n\\n  1. Compute metrics for all columns and display only `a`\\n  2. Compute and display metrics only for `a`\\n  3. Set the rolling window to 3 and display subplots for all columns\\n\\n![](https://vectorbt.pro/pvt_6739a1da/assets/images/documentation/bblocks/analyzing.svg)\\n\\nThere is nothing more satisfying than not having to write boilerplate code.\\nThanks to\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),\\nwe can shift our focus entirely to analysis, while vectorbt takes care of\\neverything else.\\n\\n### Generic accessor\\n\\nWe don\\'t have to look far to find a class that inherits\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable):\\nclass\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)\\nextends the class\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nto deliver statistics and plots for any numeric data. It\\'s a size-that-fits-\\nall class with an objective to replicate, accelerate, and extend Pandas\\' core\\nfunctionality. It implements in-house rolling, mapping, reducing, splitting,\\nplotting, and many other kinds of methods, which can be used on any Series or\\nDataFrame in the wild.\\n\\nIn a nutshell,\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)\\ndoes the following:\\n\\n  * Wraps many Numba-compiled functions from [nb](https://vectorbt.pro/pvt_6739a1da/api/generic/nb/) module and offers them as methods that mimic some of the Pandas most popular functions. Some of them have meta versions (accepting UDFs that take metadata instead of arrays) or can work on grouped data.\\n  * Wraps many data transformation utilities from [scikit-learn](https://scikit-learn.org/stable/)\\n  * Displays a range of statistics similar to [pandas.DataFrame.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)\\n  * Wraps widgets from [plotting](https://vectorbt.pro/pvt_6739a1da/api/generic/plotting/) and provides many custom plotting methods. Some of them even support interactive controls for analyzing groups of data.\\n  * Extracts basic events in form of [Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records), such as drawdowns\\n\\nSimilarly to the Base accessor, the generic accessor uses [class\\ndecorators](https://vectorbt.pro/pvt_6739a1da/api/generic/decorators/) and\\n[configs](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.nb_config)\\nto attach many Numba-compiled and scikit-learn functions at once.\\n\\nUsage is similar to `CorrStats`, except that we can call the generic accessor\\ndirectly on Pandas objects since it\\'s being directly subclassed by\\n[Vbt_DFAccessor](https://vectorbt.pro/pvt_6739a1da/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)!\\n\\n    \\n    \\n    >>> df.vbt.stats(column=\\'a\\')\\n    Start        2020-01-01 00:00:00\\n    End          2020-01-05 00:00:00\\n    Period           5 days 00:00:00\\n    Count                          5\\n    Mean                         2.0\\n    Std                     1.581139\\n    Min                            0\\n    Median                       2.0\\n    Max                            4\\n    Min Index    2020-01-01 00:00:00\\n    Max Index    2020-01-05 00:00:00\\n    Name: a, dtype: object\\n    \\n\\n## Records\\n\\n    \\n    \\n    flowchart TD;\\n        Records -.->|references| ColumnMapper;\\n        Records -.->|references| MappedArray;\\n        Records -->|inherits| Analyzable;\\n        MappedArray -.->|references| ColumnMapper;\\n        MappedArray -->|inherits| Analyzable;\\n\\nRecords are [structured\\narrays](https://numpy.org/doc/stable/user/basics.rec.html) \\\\- a NumPy array\\nthat can hold different data types, just like a Pandas DataFrame. Records have\\none big advantage over DataFrames though: they are well understood by Numba,\\nthus we can generate and use them efficiently. So, what\\'s the catch? Records\\nhave no (index) labels and the API is very limited. We also\\n[learned](https://vectorbt.pro/pvt_6739a1da/documentation/fundamentals/) that\\nvectorbt doesn\\'t like heterogenous data and prefers to work with multiple\\nhomogeneous arrays (remember how we need to split OHLC into O, H, L, and C?).\\nNevertheless, they take a very important place in our ecosystem - as\\ncontainers for event data.\\n\\nTrading is all about events: executing trades, aggregating them into\\npositions, analyzing drawdowns, and much more. Each of such events is a\\ncomplex piece of data that needs a container optimized for fast writes and\\nreads, especially inside a Numba-compiled code (but please not a list of\\ndictionaries, they are **very inefficient** ). Structured arrays is exactly\\nthe data structure we need! Each event is a record that holds all the required\\ninformation, such the column and row where it originally happened.\\n\\nBecause structured arrays are hard to analyze, there is a class just for this\\n-\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)!\\nBy subclassing\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)\\n(suprise, surprise), it wraps a structured NumPy array and provides us with\\nuseful tools for its analysis. Every\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)\\ninstance can be indexed just like a regular Pandas object and compute various\\nmetrics and plot graphs.\\n\\nLet\\'s generate the\\n[Drawdowns](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)\\nrecords for two columns of time series data:\\n\\n    \\n    \\n    >>> dd_df = pd.DataFrame({\\n    ...     \\'a\\': [10, 11, 12, 11, 12, 13, 12],\\n    ...     \\'b\\': [14, 13, 12, 11, 12, 13, 14]\\n    ... }, index=[datetime(2020, 1, 1) + timedelta(days=i) for i in range(7)])\\n    >>> drawdowns = dd_df.vbt.drawdowns\\n    >>> drawdowns.records_readable\\n       Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\\\\\\\\\n    0            0      a     2020-01-03      2020-01-04       2020-01-04   \\n    1            1      a     2020-01-06      2020-01-07       2020-01-07   \\n    2            0      b     2020-01-01      2020-01-02       2020-01-04   \\n    \\n      End Timestamp  Peak Value  Valley Value  End Value     Status  \\n    0    2020-01-05        12.0          11.0       12.0  Recovered  \\n    1    2020-01-07        13.0          12.0       12.0     Active  \\n    2    2020-01-07        14.0          11.0       14.0  Recovered \\n    \\n    >>> drawdowns[\\'b\\'].records_readable  # (1)!\\n       Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\\\\\\\\\n    0            0      b     2020-01-01      2020-01-02       2020-01-04   \\n    \\n      End Timestamp  Peak Value  Valley Value  End Value     Status  \\n    0    2020-01-07        14.0          11.0       14.0  Recovered\\n    \\n\\n  1. Select all records of the column `b` and display them in a human-readable format\\n\\nThat\\'s a lot of information! Each field is a regular NumPy array, so where do\\nwe get this rich information from? Maybe to your surprise, but the labels of\\nthe DataFrames above were auto-generated from the metadata that\\n[Drawdowns](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)\\nholds. This metadata is called a \"field config\" - a regular\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nthat describes each field (for instance,\\n[Drawdowns.field_config](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns.field_config)).\\nThis makes possible automating and enhancing the behavior of each field. Class\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records),\\nthe base class to all records classes, has many methods to read and interpret\\nthis config.\\n\\n    \\n    \\n    >>> drawdowns.status.values  # (1)!\\n    array([1, 0, 1])\\n    \\n    >>> drawdowns.get_apply_mapping_arr(\\'status\\')  # (2)!\\n    array([\\'Recovered\\', \\'Active\\', \\'Recovered\\'], dtype=object)\\n    \\n\\n  1. Raw data\\n  2. Data enhanced using the field config\\n\\n### Column mapper\\n\\nRecords are one-dimensional structured NumPy arrays. Records from multiple\\ncolumns are concatenated into a single array, so we need a mechanism to group\\nthem by column or group, for instance, to aggregate values column-wise. This\\nis a non-trivial task because finding the records that correspond to a\\nspecific column requires searching all records, which is slow when done\\nrepeatedly. The task of the class\\n[ColumnMapper](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper)\\nis to index all columns only once and cache the result (see\\n[ColumnMapper.col_map](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper.col_map)).\\nA column mapper has at least two more advantages: it allows for grouping\\ncolumns and enables efficient Indexing.\\n\\n    \\n    \\n    >>> drawdowns.col_mapper.col_map  # (1)!\\n    (array([0, 1, 2]), array([2, 1]))\\n    \\n\\n  1. An array with the indices of records ordered by column, and an array with the number of records per column\\n\\nThe produced column map means that the column `a` has two records at indices 0\\nand 1, while the column `b` is represented by only one record at index 2.\\n\\n### Mapped arrays\\n\\nIf\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)\\nis our own DataFrame for events, then\\n[MappedArray](https://vectorbt.pro/pvt_6739a1da/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray)\\nis our own Series! Each field in records can be mapped into a _mapped_ array.\\nIn fact, a mapped array is where most calculations take place. It\\'s just like\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor),\\nbut with a totally different representation of data: one-dimensional and\\nclustered instead of two-dimensional and column-wise. We can even seemingly\\nconvert between both representations. Why wouldn\\'t we then simply convert a\\nmapped array into a regular Series and do all the analysis there? There are\\nseveral reasons:\\n\\n  1. Event data is usually sparse: If 1,000,000 data points produced 50 events, it\\'s much faster to analyze 50 values then having to convert them back and deal with 9,999,950 NaNs.\\n  2. Multiple events, such as orders, can happen within the same bar, which cannot be represented in Pandas efficiently\\n\\nLet\\'s analyze the drawdown values of `drawdowns`:\\n\\n    \\n    \\n    >>> dd_ma = drawdowns.drawdown\\n    >>> dd_ma\\n    <vectorbtpro.records.mapped_array.MappedArray at 0x7ff6d8514f98>\\n    \\n    >>> dd_ma.values  # (1)!\\n    array([-0.08333333, -0.07692308, -0.21428571])\\n    \\n    >>> dd_ma.stats(column=\\'a\\')\\n    Start        2020-01-01 00:00:00\\n    End          2020-01-07 00:00:00\\n    Period           7 days 00:00:00\\n    Count                          2\\n    Mean                   -0.080128\\n    Std                     0.004533\\n    Min                    -0.083333\\n    Median                 -0.080128\\n    Max                    -0.076923\\n    Min Index    2020-01-05 00:00:00\\n    Max Index    2020-01-07 00:00:00\\n    Name: a, dtype: object\\n    \\n    >>> dd_ma.to_pd()  # (2)!\\n                       a         b\\n    2020-01-01       NaN       NaN\\n    2020-01-02       NaN       NaN\\n    2020-01-03       NaN       NaN\\n    2020-01-04       NaN       NaN\\n    2020-01-05 -0.083333       NaN\\n    2020-01-06       NaN       NaN\\n    2020-01-07 -0.076923 -0.214286\\n    \\n\\n  1. Holds three values: two from `a` and one from `b`\\n  2. Can be converted into Pandas, but at what cost?\\n\\nThanks to\\n[ColumnMapper](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper)\\nand\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),\\nwe can select rows and columns from a mapped array the same way as from\\nrecords or any regular Pandas object:\\n\\n    \\n    \\n    >>> dd_ma[\\'b\\'].values\\n    array([-0.21428571])\\n    \\n\\n## Summary\\n\\nKudos for following me all the way down here! The classes that we just covered\\nbuild a strong foundation for data analysis with vectorbt; they implement\\ndesign patterns that are encountered in most other places across the codebase,\\nwhich makes them very easy to recognize and extend. In fact, the most hard-\\ncore class\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)\\nis very similar to our `CorrStats`.\\n\\nYou\\'re now more than ready for using vectorbt, soldier\\n![ðŸŒŸ](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/1f31f.svg)\\n\\n[ Python\\ncode](https://vectorbt.pro/pvt_6739a1da/assets/jupytext/documentation/building-\\nblocks.py.txt)', start_char_idx=44550, end_char_idx=59233, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.022333518778120197),\n",
       " NodeWithScore(node=TextNode(id_='90d71dd6-6e62-4f08-93e8-636d0ed92371', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4e21ccac-4641-46a4-864e-4f722ee20b1d', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='c3d2d070c9f3fabbfed6d10b6614aa794bab09774790e5560167db16ac66ab00'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='23a68867-577b-45cd-8c81-324775b1b92b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8d20447567512dd2b4f8ca5dc82593d81f397438fb529710646c9b3d4351c5b6')}, text='You can imagine it implementing an arbitrary logic for a custom `getattr`\\noperation. It\\'s widely used in\\n[StatsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)\\nand\\n[PlotsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin)\\nto execute metrics and subplots respectively as a chain of commands. In other\\nclasses, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),\\nit\\'s also being used for accessing shortcut properties, caching attribute\\naccess, and more. It works in conjunction with\\n[deep_getattr](https://vectorbt.pro/pvt_6739a1da/api/utils/attr_/#vectorbtpro.utils.attr_.deep_getattr),\\nwhich accesses a chain of attributes provided as a specification.\\n\\nLet\\'s compute the min of the rolling mean solely using Pandas and deep\\nattribute resolution:\\n\\n    \\n    \\n    >>> sr = pd.Series([1, 2, 3, 4, 5])\\n    >>> attr_chain = [(\\'rolling\\', (3,)), \\'mean\\', \\'min\\']\\n    >>> vbt.deep_getattr(sr, attr_chain)\\n    2.0\\n    \\n\\nIf any of the above operations were done on a subclass\\n[AttrResolverMixin](https://vectorbt.pro/pvt_6739a1da/api/base/grouping/#vectorbtpro.utils.attr_.AttrResolverMixin),\\nthey could have been preprocessed and postprocessed easily.\\n\\n### Templating\\n\\nTemplates play an important role in the vectorbt\\'s ecosystem. They allow\\npostponing data resolution to a later point in time when there is more\\ninformation available. There are many different templating classes, such as\\n[Rep](https://vectorbt.pro/pvt_6739a1da/api/utils/template/#vectorbtpro.utils.template.Rep)\\nfor replacing an entire string and\\n[Sub](https://vectorbt.pro/pvt_6739a1da/api/utils/template/#vectorbtpro.utils.template.Sub)\\nfor substituting only parts of it (those beginning with `$`).\\n\\nYou can imagine templates being callbacks that are executed at some point\\nduring the execution, mostly after broadcasting or merging keyword arguments.\\nAlso, there exist functions that offer multiple potential substitution points;\\nin such case, they either attempt to substitute the template multiple times\\nuntil they succeed, or they match the template with a specific substitution id\\n(`sub_id`), if provided. The actual substitution operation is performed by\\n[substitute_templates](https://vectorbt.pro/pvt_6739a1da/api/utils/template/#vectorbtpro.utils.template.substitute_templates).\\n\\n    \\n    \\n    >>> def some_function(*args, **kwargs):\\n    ...     context = {}\\n    ...     args = vbt.substitute_templates(args, context=context, strict=False)\\n    ...     kwargs = vbt.substitute_templates(kwargs, context=context, strict=False)\\n    ...     print(args)\\n    ...     print(kwargs)\\n    ...     \\n    ...     context[\\'result\\'] = 100  # (1)!\\n    ...     args = vbt.substitute_templates(args, context=context)\\n    ...     kwargs = vbt.substitute_templates(kwargs, context=context)\\n    ...     print(args)\\n    ...     print(kwargs)\\n    \\n    >>> some_function(vbt.Rep(\\'result\\'), double_result=vbt.RepEval(\\'result * 2\\'))\\n    (Rep(template=\\'result\\', context=None, strict=None, sub_id=None),)\\n    {\\'double_result\\': RepEval(template=\\'result * 2\\', context=None, strict=None, sub_id=None)}\\n    (100,)\\n    {\\'double_result\\': 200}\\n    \\n\\n  1. New context\\n\\n## Base\\n\\n    \\n    \\n    flowchart TD;\\n        Grouper -->|inherits| Configured;\\n        ArrayWrapper -->|inherits| Configured;\\n        ArrayWrapper -->|inherits| PandasIndexer;\\n        ArrayWrapper -.->|references| Grouper;\\n        Wrapping -->|inherits| Configured;\\n        Wrapping -->|inherits| PandasIndexer;\\n        Wrapping -->|inherits| AttrResolverMixin;\\n        Wrapping -.->|references| ArrayWrapper;\\n        BaseAccessor -->|inherits| Wrapping;\\n\\nSubpackage [base](https://vectorbt.pro/pvt_6739a1da/api/base/) is the non-\\ncomputational core of vectorbt. It offers a range of modules for working with\\nand converting between Pandas and NumPy objects. In particular, it provides\\nfunctions and classes for broadcasting, combining and wrapping NumPy arrays,\\ngrouping columns, managing [MultiIndex](https://pandas.pydata.org/pandas-\\ndocs/stable/user_guide/advanced.html), and more. These operations are\\nessential for extending Pandas and replicating some of its functionality in\\ncustom classes.\\n\\n### Grouping\\n\\nSince vectorbt usually associates with processing multi-column data, where\\neach column (or \"line\") represents a separate backtesting instance, the\\nability to group those columns into some sort of groups is a must-have\\nfeature.\\n\\nClass\\n[Grouper](https://vectorbt.pro/pvt_6739a1da/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper)\\nimplements functionality to validate and build groups of any Pandas Index,\\nespecially columns. It\\'s capable of translating various metadata such as\\n[GroupBy objects](https://pandas.pydata.org/docs/reference/groupby.html) and\\ncolumn levels into special NumPy arrays that can be used by Numba-compiled\\nfunctions to aggregate multiple columns of data. This is especially useful for\\nmulti-asset portfolios where each group is composed of one or more assets.\\n\\n    \\n    \\n    >>> columns = pd.MultiIndex.from_tuples([\\n    ...     (\\'BTC-USD\\', \\'group1\\'), \\n    ...     (\\'ETH-USD\\', \\'group1\\'), \\n    ...     (\\'ADA-USD\\', \\'group2\\'),\\n    ...     (\\'SOL-USD\\', \\'group2\\')\\n    ... ], names=[\\'symbol\\', \\'group\\'])\\n    >>> vbt.Grouper(columns, \\'group\\').get_groups()\\n    array([0, 0, 1, 1])\\n    \\n\\n### Indexing\\n\\nThe main purpose of indexing in vectorbt is to provide Pandas indexing to any\\ncustom class holding Pandas-like objects, in particular, to select rows,\\ncolumns, and groups in each. This is done by forwarding a Pandas indexing\\noperation to each Pandas-like object and instantiating the class using them,\\nwhich is fairly easy using\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured).\\nThis way, one can index complex classes with dozens of Pandas-like objects\\nusing a single command.\\n\\nThe main indexer class\\n[PandasIndexer](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer)\\nmimics a regular Pandas object by exposing properties\\n[PandasIndexer.iloc](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.iloc),\\n[PandasIndexer.loc](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.loc),\\nand\\n[PandasIndexer.xs](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.xs).\\nAll we have to do is to subclass this class and override\\n[IndexingBase.indexing_func](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.IndexingBase.indexing_func),\\nwhich should take `pd_indexing_func`, apply it on each Pandas-like object, and\\ninitialize a new instance.\\n\\nLet\\'s extend our newly created `CorrStats` class with Pandas indexing:\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Configured, vbt.PandasIndexer):\\n    ...     def __init__(self, obj1, obj2):\\n    ...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)\\n    ...         vbt.PandasIndexer.__init__(self)\\n    ...\\n    ...         self._obj1 = obj1\\n    ...         self._obj2 = obj2\\n    ...\\n    ...     def indexing_func(self, pd_indexing_func):  # (1)!\\n    ...         return self.replace(\\n    ...             obj1=pd_indexing_func(self.obj1),\\n    ...             obj2=pd_indexing_func(self.obj2)\\n    ...         )\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):\\n    ...         if isinstance(self.obj1, pd.Series):\\n    ...             return self.obj1.corr(self.obj2)\\n    ...         return self.obj1.corrwith(self.obj2)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         return self.obj1.rolling(window).corr(self.obj2)\\n    \\n    >>> corrstats = CorrStats(df1, df2)\\n    >>> corrstats.corr()\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> corrstats.loc[\\'2020-01-01\\':\\'2020-01-03\\', \\'a\\'].corr()  # (2)!\\n    0.24019223070763066\\n    \\n\\n  1. Here, `pd_indexing_func` is just `lambda x: x.loc[key]`\\n  2. Running the correlation coefficient on a subset of data\\n\\nWe just indexed two Pandas objects as a single entity. Yay!\\n\\n### Wrapping\\n\\nRemember how vectorbt specializes at taking a Pandas object, extracting its\\nNumPy array, processing the array, and converting the results back into a\\nPandas format? The last part is done by the class\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper),\\nwhich captures all the necessary metadata, such as the index, columns, and\\nnumber of dimensions, and exposes methods such as\\n[ArrayWrapper.wrap](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)\\nto convert a NumPy object back into a Pandas format.\\n\\nClass\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\ncombines many concepts we introduced earlier to behave just like a\\n(supercharged) Pandas object. In particular, it uses Grouping to build and\\nmanage groups of columns, and Indexing to select rows, columns, and groups\\nusing Pandas indexing. Probably the most powerful features of an array wrapper\\nis 1) the ability to behave like a grouped object, which isn\\'t possible with\\nPandas alone, and 2) the ability to translate a Pandas indexing operation to a\\nrange of integer arrays that can be used to index NumPy arrays. The latter\\nallows indexing without the need to hold Pandas objects, only the wrapper.\\n\\nWe can construct a wrapper in multiple ways, the easiest being using a Pandas\\nobject:\\n\\n    \\n    \\n    >>> df = pd.DataFrame({\\n    ...     \\'a\\': range(0, 5),\\n    ...     \\'b\\': range(5, 10),\\n    ...     \\'c\\': range(10, 15),\\n    ...     \\'d\\': range(15, 20)\\n    ... }, index=index)\\n    >>> wrapper = vbt.ArrayWrapper.from_obj(df)\\n    >>> print(wrapper)\\n    ArrayWrapper(\\n        index=<DatetimeIndex object at 0x7ff6d8528d68 of shape (5,)>,\\n        columns=<Index object at 0x7ff6d857fcc0 of shape (4,)>,\\n        ndim=2,\\n        freq=None,\\n        column_only_select=None,\\n        group_select=None,\\n        grouped_ndim=None,\\n        grouper=Grouper(\\n            index=<Index object at 0x7ff6d857fcc0 of shape (4,)>,\\n            group_by=None,\\n            allow_enable=True,\\n            allow_disable=True,\\n            allow_modify=True\\n        )\\n    )\\n    \\n\\nLet\\'s create a function that sums all elements over each column using NumPy\\nand returns a regular Pandas object:\\n\\n    \\n    \\n    >>> def sum_per_column(df):\\n    ...     wrapper = vbt.ArrayWrapper.from_obj(df)\\n    ...     result = np.sum(df.values, axis=0)\\n    ...     return wrapper.wrap_reduced(result)\\n    \\n    >>> sum_per_column(df)\\n    a    10\\n    b    35\\n    c    60\\n    d    85\\n    dtype: int64\\n    \\n\\nThe function above is already 20x faster than Pandas\\n![ðŸ¤¯](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/1f92f.svg)\\n\\n    \\n    \\n    >>> big_df = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    \\n    >>> %timeit big_df.sum()\\n    2.52 ms Â± 3.5 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\\n    \\n    >>> %timeit sum_per_column(big_df)\\n    428 Âµs Â± 1.5 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\\n    \\n\\nSince\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\ncan manage groups of columns, let\\'s adapt our function to sum all elements\\nover each group of columns:\\n\\n    \\n    \\n    >>> def sum_per_group(df, group_by):\\n    ...     wrapper = vbt.ArrayWrapper.from_obj(df, group_by=group_by)\\n    ...     results = []\\n    ...     for group_idxs in wrapper.grouper.yield_group_idxs():\\n    ...         group_result = np.sum(df.values[:, group_idxs])\\n    ...         results.append(group_result)\\n    ...     return wrapper.wrap_reduced(results)\\n    \\n    >>> sum_per_group(df, False)  # (1)!\\n    a    10\\n    b    35\\n    c    60\\n    d    85\\n    dtype: int64\\n    \\n    >>> sum_per_group(df, True)  # (2)!\\n    190\\n    \\n    >>> group_by = pd.Index([\\'group1\\', \\'group1\\', \\'group2\\', \\'group2\\'])\\n    >>> sum_per_group(df, group_by)  # (3)!\\n    group1     45\\n    group2    145\\n    dtype: int64\\n    \\n\\n  1. No grouping (one group per column)\\n  2. One group with all columns\\n  3. Multiple groups with multiple columns\\n\\nTo avoid creating multiple array wrappers with the same metadata, there is the\\nclass\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping),\\nwhich binds a single instance of\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\nto manage an arbitrary number of shape-compatible array-like objects. Instead\\nof accepting multiple Pandas objects, it takes an array wrapper, and all other\\nobjects and arrays in any format (preferably NumPy), and wraps them using this\\nwrapper. Additionally, any\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nsubclass can utilize its wrapper to perform Pandas indexing on any kind of\\nobjects, including NumPy arrays, because\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\ncan translate a Pandas indexing operation into universal row, column, and\\ngroup indices.\\n\\nComing back to our `CorrStats` class. There are two issues with the current\\nimplementation:\\n\\n  1. Both metrics require Pandas objects of identical layouts as inputs\\n  2. Both metrics are computed using Pandas, which is rather slow\\n\\nLet\\'s upgrade our `CorrStats` class to work on NumPy arrays and with an array\\nwrapper:\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Wrapping):\\n    ...     _expected_keys = vbt.Wrapping._expected_keys | {\"obj1\", \"obj2\"}  # (1)!\\n    ...\\n    ...     @classmethod\\n    ...     def from_objs(cls, obj1, obj2):  # (2)!\\n    ...         (obj1, obj2), wrapper = vbt.broadcast(\\n    ...             obj1, obj2, \\n    ...             to_pd=False,\\n    ...             return_wrapper=True\\n    ...         )\\n    ...         return cls(wrapper, obj1, obj2)\\n    ...\\n    ...     def __init__(self, wrapper, obj1, obj2):\\n    ...         vbt.Wrapping.__init__(self, wrapper, obj1=obj1, obj2=obj2)\\n    ...\\n    ...         self._obj1 = vbt.to_2d_array(obj1)\\n    ...         self._obj2 = vbt.to_2d_array(obj2)\\n    ...\\n    ...     def indexing_func(self, pd_indexing_func, **kwargs):  # (3)!\\n    ...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)\\n    ...         new_wrapper = wrapper_meta[\"new_wrapper\"]\\n    ...         row_idxs = wrapper_meta[\"row_idxs\"]\\n    ...         col_idxs = wrapper_meta[\"col_idxs\"]\\n    ...         return self.replace(\\n    ...             wrapper=new_wrapper,\\n    ...             obj1=self.obj1[row_idxs, :][:, col_idxs],\\n    ...             obj2=self.obj2[row_idxs, :][:, col_idxs]\\n    ...         )\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):  # (4)!\\n    ...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    \\n\\n  1.', start_char_idx=15049, end_char_idx=30629, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.01421490952438097),\n",
       " NodeWithScore(node=TextNode(id_='4e21ccac-4641-46a4-864e-4f722ee20b1d', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='68d0cdf5-4284-42ee-ad0d-44e531f2e967', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/cookbook-persistence.txt', 'file_name': 'cookbook-persistence.txt', 'file_type': 'text/plain', 'file_size': 3968, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='725fb4e117de05f69ff888f9a426d93861552dcd079222d8651f48cc8d958872'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='90d71dd6-6e62-4f08-93e8-636d0ed92371', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8fd4f68faabe3461f36b7a90c97b320901df4d494c3b2ad9d96b5fd958f150e6')}, text='#  Building blocks\\n\\nIn what follows, we will look at sub-packages, modules, and especially classes\\nthat act as building blocks for more advanced functionalities distributed\\nacross vectorbt, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).\\nFor illustration, we will gradually build our custom class `CorrStats` that\\nwill let us analyze the correlation between two arrays in the most performant\\nand flexible way\\n![ðŸ§ ](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/1f9e0.svg)\\n\\n## Utilities\\n\\n    \\n    \\n    flowchart TD;\\n        Config -->|inherits| Pickleable;\\n        Config -->|inherits| Prettified;\\n        Configured -->|inherits| Pickleable;\\n        Configured -->|inherits| Prettified;\\n        Configured -.->|references| Config;\\n        AttrResolverMixin;\\n\\n(Reload the page if the diagram doesn\\'t show up)\\n\\nVectorBT PRO deploys a modular project structure that is composed of a range\\nof subpackages. Each subpackage is applicable to a certain area of analysis.\\n\\nSubpackage [utils](https://vectorbt.pro/pvt_6739a1da/api/utils/) contains a\\nset of utilities powering every part of vectorbt\\n![âš¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/26a1.svg)\\nThey are loosely connected and provide small but powerful re-usable code\\nsnippets that can be used independently of other functionality.\\n\\nInfo\\n\\nThe main reason why we don\\'t import third-party packages but implement many\\nutilities from scratch is because we want to retain full control over\\nexecution and code quality.\\n\\n### Formatting\\n\\nVectorBT PRO implements its own formatting engine that can pretty-print any\\nPython object. It\\'s far more superior to formatting with\\n[JSON](https://en.wikipedia.org/wiki/JSON) because it respects native Python\\ndata types and injects some smart formatting logic when it comes to more\\nstructured data types, such as `np.dtype` and `namedtuple`. Often, we can even\\nconvert the string back into Python using `eval`.\\n\\nLet\\'s beautify a nested dictionary using\\n[prettify](https://vectorbt.pro/pvt_6739a1da/api/utils/formatting/#vectorbtpro.utils.formatting.prettify)\\nand convert the string back into an object:\\n\\n    \\n    \\n    >>> import vectorbtpro as vbt\\n    >>> import numpy as np\\n    >>> import pandas as pd\\n    >>> from datetime import datetime, timedelta\\n    \\n    >>> dct = {\\'planet\\' : {\\'has\\': {\\'plants\\': \\'yes\\', \\'animals\\': \\'yes\\', \\'cryptonite\\': \\'no\\'}, \\'name\\': \\'Earth\\'}}\\n    >>> print(vbt.prettify(dct))\\n    {\\n        \\'planet\\': {\\n            \\'has\\': {\\n                \\'plants\\': \\'yes\\',\\n                \\'animals\\': \\'yes\\',\\n                \\'cryptonite\\': \\'no\\'\\n            },\\n            \\'name\\': \\'Earth\\'\\n        }\\n    }\\n    \\n    >>> eval(vbt.prettify(dct)) == dct\\n    True\\n    \\n\\nHint\\n\\nWondering why we used `vbt.prettify` instead of\\n`vbt.utils.formatting.prettify`? Any utility that may prove useful to the end\\nuser can be accessed directly from `vbt`.\\n\\nTo see which utilities are accessible from the root of the package, visit\\n[vectorbtpro/utils/__init__.py](https://github.com/polakowo/vectorbt.pro/blob/main/vectorbtpro/utils/__init__.py)\\nor any other subpackage, and look for the objects that are listed in\\n`__all__`.\\n\\nClass\\n[Prettified](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.formatting.Prettified)\\nimplements the abstract method\\n[Prettified.prettify](https://vectorbt.pro/pvt_6739a1da/api/utils/formatting/#vectorbtpro.utils.formatting.Prettified.prettify),\\nwhich a subclass can override to pretty-print an instance using\\n[prettify](https://vectorbt.pro/pvt_6739a1da/api/utils/formatting/#vectorbtpro.utils.formatting.prettify).\\nRead below to learn how instances of various classes can be introspected using\\nthis method.\\n\\n### Pickling\\n\\nPickling is the process of converting a Python object into a byte stream to\\nstore it in a file/database. Class\\n[Pickleable](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable)\\nenables pickling of objects of any complexity using\\n[Dill](https://dill.readthedocs.io/en/latest/) (or\\n[pickle](https://docs.python.org/3/library/pickle.html) if Dill is not\\ninstalled). Each of its subclasses inherits ready-to-use methods for\\nserializing, de-serializing, saving to a file, and loading from a file. This\\nis truly amazing because it allows us to persist objects holding any type of\\ndata, including instances of\\n[Data](https://vectorbt.pro/pvt_6739a1da/api/data/base/#vectorbtpro.data.base.Data)\\nand\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).\\n\\n### Configuring\\n\\nVectorBT PRO heavily relies upon automation based on some sort of\\nspecification. The specification for most repetitive tasks is usually stored\\ninside so-called \"configs\", which act as settings for a certain task, a data\\nstructure, or even a class. This makes most places in vectorbt transparent and\\neasily traversable and changeable programmatically.\\n\\nClass\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nis a dictionary on steroids: it extends the Python\\'s `dict` with various\\nconfiguration features, such as frozen keys, read-only values, accessing keys\\nvia the dot notation, and nested updates. The most notable feature is the\\nability to reset a config to its initial state and even make checkpoints,\\nwhich is particularly useful for settings. In addition, since\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\ninherits\\n[Pickleable](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable),\\nwe can save any configuration to disk, while subclassing\\n[Prettified](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.formatting.Prettified)\\nallows us to beautify it (by the way, this approach is being used to generate\\nthe API reference):\\n\\n    \\n    \\n    >>> print(vbt.Records.field_config)\\n    Config(\\n        dtype=None,\\n        settings={\\n            \\'id\\': {\\n                \\'name\\': \\'id\\',\\n                \\'title\\': \\'Id\\'\\n            },\\n            \\'col\\': {\\n                \\'name\\': \\'col\\',\\n                \\'title\\': \\'Column\\',\\n                \\'mapping\\': \\'columns\\'\\n            },\\n            \\'idx\\': {\\n                \\'name\\': \\'idx\\',\\n                \\'title\\': \\'Timestamp\\',\\n                \\'mapping\\': \\'index\\'\\n            }\\n        }\\n    )\\n    \\n\\nConfigs are very common structures in vectorbt. There are three main types of\\nconfigs (that either subclass or partially call\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config))\\nused throughout vectorbt:\\n\\n  1. [ReadonlyConfig](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.ReadonlyConfig) for configurations that aren\\'t meant to be modified, such as [nb_config](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.nb_config), which is being used to attach a number of Numba-compiled functions to [GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor) only once upon importing vectorbt, so if it were modifiable it wouldn\\'t have any effect anyway.\\n  2. [HybridConfig](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.HybridConfig) for configuration that are meant to be modified. The best examples are [Portfolio.metrics](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.metrics) and [Portfolio.subplots](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.subplots), which contain all the metrics and subplots supported by [Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio); they can be easily changed and extended, and then reset if you did some breaking changes.\\n  3. [SettingsConfig](https://vectorbt.pro/pvt_6739a1da/api/_settings/#vectorbtpro._settings.SettingsConfig) holding the global settings defined in [_settings](https://vectorbt.pro/pvt_6739a1da/api/_settings/). It\\'s a custom subclass of [Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config) that is capable of changing Plotly themes and converting any sub-configs of type `dict` to smaller settings accessible via the dot notation (`vbt.settings.portfolio.log` instead of `vbt.settings[\\'portfolio\\'][\\'log\\']`)\\n\\nA config can be created like a regular `dict`. To provide options for this\\nconfig, use the `options_` argument (notice the trailing underscore):\\n\\n    \\n    \\n    >>> cfg = vbt.Config(\\n    ...     hello=\"world\",\\n    ...     options_=dict(readonly=True)\\n    ... )\\n    >>> print(cfg)\\n    Config(\\n        hello=\\'world\\'\\n    )\\n    \\n    >>> cfg[\"change\"] = \"something\"\\n    TypeError: Config is read-only\\n    \\n\\nApart from the use cases listed above,\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nis also being used by the class\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured),\\nwhich is a base class to most core classes in vectorbt. It\\'s a read-only class\\nthat holds a config of type\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nwith all arguments passed during the initialization. Whenever we initialize\\nany subclass of\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured),\\nany named arguments we passed to the initializer (`__init__`) are stored\\ninside\\n[Configured.config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured.config).\\nThis way, the created instance is described and managed solely by its config:\\n\\n  * We can copy/modify the config and pass it to the class to initialize another instance (`new_instance = ConfiguredClass(**old_instance.config)`)\\n  * We can pickle the class and the config to pickle the entire instance\\n  * We can compare instances based on their classes and configs\\n\\nThe main requirement for all of this to work properly is **immutability**. And\\nhere we have arrived at the first very important design decision: most classes\\nin vectorbt are meant to be immutable (read-only) and it\\'s discouraged to\\nchange any attribute unless it\\'s listed in a special variable called\\n`_writeable_attrs`. There are multiple reasons why we require immutability:\\n\\n  1. Immutable instances can be easily recreated and manipulated using their configs\\n  2. Immutable instances are side effect free and can have cached attributes\\n  3. Immutable instances can be hashed\\n\\nLet\\'s create our custom class that returns some correlation statistics of two\\narrays. In particular, it will compute the Pearson correlation coefficient and\\nits rolling version using Pandas:\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Configured):\\n    ...     def __init__(self, obj1, obj2):\\n    ...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)\\n    ...\\n    ...         self._obj1 = obj1\\n    ...         self._obj2 = obj2\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):\\n    ...         if isinstance(self.obj1, pd.Series):\\n    ...             return self.obj1.corr(self.obj2)\\n    ...         return self.obj1.corrwith(self.obj2)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         return self.obj1.rolling(window).corr(self.obj2)\\n    \\n\\nThis is how most configured classes in vectorbt, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),\\nare designed. Any argument that is being passed to `CorrStats` is forwarded\\ndown to\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured)\\nto initialize a new config:\\n\\n    \\n    \\n    >>> index = [datetime(2020, 1, 1) + timedelta(days=i) for i in range(5)]\\n    >>> df1 = pd.DataFrame({\\n    ...     \\'a\\': [1, 5, 2, 4, 3],\\n    ...     \\'b\\': [3, 2, 4, 1, 5]\\n    ... }, index=index)\\n    >>> df2 = pd.DataFrame({\\n    ...     \\'a\\': [1, 2, 3, 4, 5],\\n    ...     \\'b\\': [5, 4, 3, 2, 1]\\n    ... }, index=index)\\n    \\n    >>> corrstats = CorrStats(df1, df2)\\n    >>> print(corrstats.config)\\n    Config(\\n        obj1=<DataFrame object at 0x7fd490461400 of shape (5, 2)>,\\n        obj2=<DataFrame object at 0x7fd490461240 of shape (5, 2)>\\n    )\\n    \\n\\nAccess to any attribute is read-only: whenever we try to set a read-only\\nproperty or modify the config, an (expected) error will be thrown:\\n\\n    \\n    \\n    >>> df3 = pd.DataFrame({\\n    ...     \\'a\\': [3, 2, 1, 5, 4],\\n    ...     \\'b\\': [4, 5, 1, 2, 3]\\n    ... }, index=index)\\n    >>> corrstats.obj1 = df3\\n    AttributeError: can\\'t set attribute\\n    \\n    >>> corrstats.config[\\'obj1\\'] = df3\\n    TypeError: Config is read-only\\n    \\n\\nHowever, it won\\'t (and can\\'t) throw an error when setting a private attribute\\n(with a leading underscore) or if any of the attributes were modified in\\nplace, which is a common pitfall you should avoid.\\n\\n    \\n    \\n    >>> corrstats._obj1 = df3  # (1)!\\n    >>> corrstats.obj1.iloc[:] = df3  # (2)!\\n    \\n\\n  1. This would work, but at what cost?\\n  2. This would work too, unfortunately\\n\\nWarning\\n\\nvectorbt assumes that the data in a configured instance always stays the same.\\nWhenever there is a change to data, vectorbt won\\'t register it and likely\\ndeliver erroneous results at some point in time.\\n\\nThe only approved way to change any data in an instance is to create another\\ninstance!\\n\\nTo change anything, pass the new data to\\n[Configured.replace](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured.replace),\\nwhich takes the same arguments as the class but in the keyword-only format,\\nmerges them over the old config, and passes as keyword arguments to the class\\nfor instantiation.\\n\\n    \\n    \\n    >>> new_corrstats = corrstats.replace(obj1=df3)\\n    >>> new_corrstats.obj1  # (1)!\\n                a  b\\n    2020-01-01  3  4\\n    2020-01-02  2  5\\n    2020-01-03  1  1\\n    2020-01-04  5  2\\n    2020-01-05  4  3\\n    \\n    >>> new_corrstats.obj2  # (2)!\\n                a  b\\n    2020-01-01  1  5\\n    2020-01-02  2  4\\n    2020-01-03  3  3\\n    2020-01-04  4  2\\n    2020-01-05  5  1\\n    \\n\\n  1. `df1` has been replaced with `df3`\\n  2. `df2` remains untouched\\n\\nSince all of our data is now stored inside a config, we can perform many\\nactions on the instance as if we performed them on the config itself, such as\\nsaving to disk (thanks to Pickling):\\n\\n    \\n    \\n    >>> corrstats.save(\\'corrstats\\')  # (1)!\\n    \\n    >>> corrstats = CorrStats.load(\\'corrstats\\')\\n    \\n\\n  1. Saves the instance along with both objects\\n\\n### Attribute resolution\\n\\nAttribute resolution is handy when it comes to accessing attributes based on\\nstrings or some other logic, which is realized by the mixin\\n[AttrResolverMixin](https://vectorbt.pro/pvt_6739a1da/api/base/grouping/#vectorbtpro.utils.attr_.AttrResolverMixin).\\nYou can imagine it implementing an arbitrary logic for a custom `getattr`\\noperation. It\\'s widely used in\\n[StatsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)\\nand\\n[PlotsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin)\\nto execute metrics and subplots respectively as a chain of commands. In other\\nclasses, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),\\nit\\'s also being used for accessing shortcut properties, caching attribute\\naccess, and more.', start_char_idx=0, end_char_idx=15717, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.0005152954923357689),\n",
       " NodeWithScore(node=TextNode(id_='68d0cdf5-4284-42ee-ad0d-44e531f2e967', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/cookbook-persistence.txt', 'file_name': 'cookbook-persistence.txt', 'file_type': 'text/plain', 'file_size': 3968, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='76f89382-71ad-4766-9278-5bc0b5fe5a00', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/cookbook-persistence.txt', 'file_name': 'cookbook-persistence.txt', 'file_type': 'text/plain', 'file_size': 3968, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='cfa9f4ed0c291f348860dea229f8991681af407a2db15b111589f7e1699f7920'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4e21ccac-4641-46a4-864e-4f722ee20b1d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='dd46af30849aa5c66ef17bff9d664dcb3152503f83d435591d0c23d373dcfc20')}, text='#  Persistence\\n\\nAny Python object can be serialized and saved to disk as a pickle file with\\n[save](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.save).\\n\\nSave a dict to a file\\n\\n    \\n    \\n    cache = dict(data=data, indicator=indicator, pf=pf)\\n    vbt.save(cache, \"cache.pickle\")\\n    \\n\\nImportant\\n\\nIf a file with the same name already exists, it will be overridden.\\n\\nA pickle file can then be loaded back and deserialized with\\n[load](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.load).\\n\\nLoad the dict back\\n\\n    \\n    \\n    cache = vbt.load(\"cache.pickle\")\\n    \\n\\nNote\\n\\nThe file can be read in another Python environment and even on another machine\\n(such as in cloud), just make sure that the Python and package versions on\\nboth ends are approximately the same.\\n\\nPickle files usually take a considerable amount of space, to reduce it\\ncompression can be used. The most recommended compression algorithm for binary\\nfiles is [blosc](https://github.com/Blosc/c-blosc). To later load the\\ncompressed file, pass the `compression` argument in the exact same way to the\\nloader, or simply append the \".blosc\" extension to the filename for the loader\\nto recognize it automatically. The supported algorithms and their possible\\nextensions are listed under `extensions` in\\n[settings.pickling](https://vectorbt.pro/pvt_6739a1da/api/_settings/#vectorbtpro._settings.pickling).\\n\\nSpecify the compression explicitly\\n\\n    \\n    \\n    vbt.save(cache, \"cache.pickle\", compression=\"blosc\")\\n    cache = vbt.load(\"cache.pickle\", compression=\"blosc\")\\n    \\n\\nSpecify the compression implicitly\\n\\n    \\n    \\n    vbt.save(cache, \"cache.pickle.blosc\")\\n    cache = vbt.load(\"cache.pickle.blosc\")\\n    \\n\\nThose VBT objects that subclass\\n[Pickleable](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable)\\ncan also be saved individually. Benefit: the name of the class and optionally\\nthe compression algorithm will be packed into the filename by default to\\nsimplify loading. The object can be loaded back using the `load()` method of\\nthe object\\'s class.\\n\\nSave a portfolio under \\'Portfolio.pickle.blosc\\' and load it back\\n\\n    \\n    \\n    pf.save(compression=\"blosc\")\\n    pf = vbt.PF.load()\\n    \\n\\nIf a VBT object was saved with an older package version and upon loading with\\na newer version an error is thrown (for example, due to a different order of\\nthe arguments), the object can still be reconstructed by creating and\\nregistering a\\n[RecInfo](https://vectorbt.pro/pvt_6739a1da/api/utils/pickling/#vectorbtpro.utils.pickling.RecInfo)\\ninstance before loading.\\n\\nReconstruct an older BinanceData instance\\n\\n    \\n    \\n    def modify_state(rec_state):  # (1)!\\n        return vbt.RecState(\\n            init_args=rec_state.init_args,\\n            init_kwargs=rec_state.init_kwargs,\\n            attr_dct=rec_state.attr_dct,\\n        )\\n    \\n    rec_info = vbt.RecInfo(\\n        vbt.get_id_from_class(vbt.BinanceData),\\n        vbt.BinanceData,\\n        modify_state\\n    )\\n    rec_info.register()\\n    data = vbt.BinanceData.load()\\n    \\n\\n  1. Change the arguments to be passed to the class here\\n\\nIf there are issues with saving an instance of a specific class, set the\\nreconstruction id `_rec_id` with any string and then reconstruct the object\\nusing this id (first argument of `RecInfo`).\\n\\nSet a custom identifier to a class and reconstruct its instance using another\\nclass\\n\\n    \\n    \\n    class MyClass1(vbt.Configured):  # (1)!\\n        _rec_id = \"MyClass\"\\n        ...\\n    \\n    my_obj = MyClass1()\\n    vbt.save(my_obj, \"my_obj\")\\n    \\n    # (2)!\\n    \\n    class MyClass2(vbt.Configured):\\n        ...\\n    \\n    rec_info = vbt.RecInfo(\"MyClass\", MyClass2)\\n    rec_info.register()\\n    my_obj = vbt.load(\"my_obj\")  # (3)!\\n    \\n\\n  1. `vbt.Configured` can be replaced by any other VBT class that subclasses it\\n  2. ...restart the kernel...\\n  3. The resulting object is of the class `MyClass2` even though we saved it as `MyClass1`!', start_char_idx=0, end_char_idx=3966, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=-0.03404185229157019)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index2.as_chat_engine(chat_mode=\"react\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'How to move stop loss to breakeven after the first take profit is hit in a trading strategy?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: To move the stop loss to breakeven after the first take profit is hit in a trading strategy, you can use a conditional statement to check if the take profit condition has been met. If it has, you can then update the stop loss level to the entry price. This will ensure that you lock in profits and protect your position from potential losses.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools.\n",
      "Answer: To move the stop loss to breakeven after the first take profit is hit in a trading strategy, you can use a conditional statement to check if the take profit condition has been met. If it has, you can then update the stop loss level to the entry price. This will ensure that you lock in profits and protect your position from potential losses.\n",
      "\u001b[0mTo move the stop loss to breakeven after the first take profit is hit in a trading strategy, you can use a conditional statement to check if the take profit condition has been met. If it has, you can then update the stop loss level to the entry price. This will ensure that you lock in profits and protect your position from potential losses.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"In my strategy I have 4 take profits and stop loss. How can I move stop loss to breakeven after the first take profit is hit?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Certainly! Here's an example code snippet that demonstrates how to move the stop loss to breakeven after the first take profit is hit in a trading strategy:\n",
      "\n",
      "```python\n",
      "# Initialize variables\n",
      "take_profit_levels = [100, 200, 300, 400]  # List of take profit levels\n",
      "stop_loss = 50  # Stop loss level\n",
      "entry_price = 150  # Entry price\n",
      "current_price = 250  # Current price\n",
      "\n",
      "# Check if the first take profit level is hit\n",
      "if current_price >= take_profit_levels[0]:\n",
      "    # Move stop loss to breakeven\n",
      "    stop_loss = entry_price\n",
      "\n",
      "# Print the updated stop loss level\n",
      "print(\"Updated stop loss:\", stop_loss)\n",
      "```\n",
      "\n",
      "In this code, we have a list `take_profit_levels` that contains the desired take profit levels. We also have the `stop_loss` level and the `entry_price` of the trade. The `current_price` represents the current market price.\n",
      "\n",
      "The code checks if the `current_price` is greater than or equal to the first take profit level (`take_profit_levels[0]`). If it is, it means that the first take profit level is hit. In that case, the `stop_loss` is updated to the `entry_price`, effectively moving it to breakeven.\n",
      "\n",
      "Finally, the code prints the updated stop loss level.\n",
      "\n",
      "Please note that this is a simplified example, and you may need to adapt it to your specific trading strategy and data structures.\n",
      "\u001b[0mCertainly! Here's an example code snippet that demonstrates how to move the stop loss to breakeven after the first take profit is hit in a trading strategy:\n",
      "\n",
      "```python\n",
      "# Initialize variables\n",
      "take_profit_levels = [100, 200, 300, 400]  # List of take profit levels\n",
      "stop_loss = 50  # Stop loss level\n",
      "entry_price = 150  # Entry price\n",
      "current_price = 250  # Current price\n",
      "\n",
      "# Check if the first take profit level is hit\n",
      "if current_price >= take_profit_levels[0]:\n",
      "    # Move stop loss to breakeven\n",
      "    stop_loss = entry_price\n",
      "\n",
      "# Print the updated stop loss level\n",
      "print(\"Updated stop loss:\", stop_loss)\n",
      "```\n",
      "\n",
      "In this code, we have a list `take_profit_levels` that contains the desired take profit levels. We also have the `stop_loss` level and the `entry_price` of the trade. The `current_price` represents the current market price.\n",
      "\n",
      "The code checks if the `current_price` is greater than or equal to the first take profit level (`take_profit_levels[0]`). If it is, it means that the first take profit level is hit. In that case, the `stop_loss` is updated to the `entry_price`, effectively moving it to breakeven.\n",
      "\n",
      "Finally, the code prints the updated stop loss level.\n",
      "\n",
      "Please note that this is a simplified example, and you may need to adapt it to your specific trading strategy and data structures.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Can you now show me the code of the strategy we discussed?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'What is Vectorbt Pro?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Vectorbt Pro is a computational library that provides modules for processing and plotting time series and numeric data. It includes an arsenal of Numba-compiled functions for accelerating and extending Pandas. It also offers builder mixins, which are classes that allow specific functionality to be built from a class's attributes. Vectorbt Pro aims to improve performance by using NumPy and Numba for computations, while still maintaining the ability to present results in a Pandas format.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools.\n",
      "Answer: Vectorbt Pro is a computational library that provides modules for processing and plotting time series and numeric data. It includes Numba-compiled functions for accelerating and extending Pandas, as well as builder mixins for building specific functionality. Vectorbt Pro aims to improve performance by using NumPy and Numba for computations while still maintaining compatibility with Pandas.\n",
      "\u001b[0mVectorbt Pro is a computational library that provides modules for processing and plotting time series and numeric data. It includes Numba-compiled functions for accelerating and extending Pandas, as well as builder mixins for building specific functionality. Vectorbt Pro aims to improve performance by using NumPy and Numba for computations while still maintaining compatibility with Pandas.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"What do you know about vectorbt pro?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='23a68867-577b-45cd-8c81-324775b1b92b', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='90d71dd6-6e62-4f08-93e8-636d0ed92371', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='503923925f0ff1b8594861618671011d019feecef6f8236bd14272ed1166404e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7f856c6e-02ee-4d7d-bc80-c1499f8878dd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d5752d2851ff8275ac2741c9dd5dd167e6f0d8a661f56fc4e723f2174ab0f5be')}, text='...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    \\n\\n  1. We need to specify the argument names that we are about to pass to [Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) (or set `_expected_keys=None` to disable)\\n  2. Convenient class method to broadcast objects, create a wrapper, and pass everything to the constructor\\n  3. Indexing method that uses the wrapper to translate `pd_indexing_func` to an array of selected rows and columns, applies them on both NumPy arrays, and creates a new `CorrStats` instance\\n  4. Computation is done on NumPy objects and the result is converted into Pandas\\n\\nAs you might have noticed, we replaced the superclasses\\n[Configured](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Configured)\\nand\\n[PandasIndexer](https://vectorbt.pro/pvt_6739a1da/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer)\\nwith a single superclass\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping),\\nwhich already inherits them both. Another change applies to the arguments\\ntaken by `CorrStats`: instead of taking two Pandas objects, it now takes\\n`wrapper` of type\\n[ArrayWrapper](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)\\nalong with the NumPy arrays `obj1` and `obj2`. This has several benefits:\\nwe\\'re keeping Pandas metadata consistent and managed by a single variable,\\nwhile all actions are efficiently performed using NumPy alone. Whenever there\\nis a need to present the findings, we can call\\n[ArrayWrapper.wrap_reduced](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap_reduced)\\nand\\n[ArrayWrapper.wrap](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)\\nto transform them back into a Pandas format, which is done inside the methods\\n`CorrStats.corr` and `CorrStats.rolling_corr` respectively.\\n\\nSince we don\\'t want to force ourselves and the user to create an array wrapper\\nmanually, we also implemented the class method `CorrStats.from_objs`, which\\nbroadcasts both arrays and instantiates `CorrStats`. This way, we can provide\\narray-like objects of any kind and `CorrStats` will automatically build the\\nwrapper for us. Let\\'s illustrate this by computing the correlation coefficient\\nfor `df1` and `df2`, and then for `df1` and parametrized `df2`:\\n\\n    \\n    \\n    >>> df1.corrwith(df2)  # (1)!\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2)\\n    >>> corrstats.corr()  # (2)!\\n    a    0.3\\n    b   -0.3\\n    dtype: float64\\n    \\n    >>> df2_sh = vbt.pd_acc.concat(\\n    ...     df2, df2.vbt.shuffle(seed=42), \\n    ...     keys=[\\'plain\\', \\'shuffled\\'])\\n    >>> df2_sh \\n                  plain    shuffled   \\n                   a  b        a  b\\n    2020-01-01     1  5        2  2\\n    2020-01-02     2  4        5  4\\n    2020-01-03     3  3        3  3\\n    2020-01-04     4  2        1  5\\n    2020-01-05     5  1        4  1\\n    \\n    >>> df1.corrwith(df2_sh)  # (3)!\\n    ValueError: cannot join with no overlapping index names\\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2_sh)\\n    >>> corrstats.corr()  # (4)!\\n    plain     a    0.3\\n              b   -0.3\\n    shuffled  a    0.4\\n              b   -0.9\\n    dtype: float64\\n    \\n\\n  1. Using Pandas\\n  2. Using our class\\n  3. Pandas doesn\\'t know how to join this information\\n  4. vectorbt knows exactly what to do thanks to smart broadcasting\\n\\nHere\\'s why we ditched Pandas in favor of Numba:\\n\\n    \\n    \\n    >>> big_df1 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    >>> big_df2 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))\\n    \\n    >>> %timeit big_df1.rolling(10).corr(big_df2)  # (1)!\\n    271 ms Â± 13.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\\n    \\n    >>> corrstats = CorrStats.from_objs(big_df1, big_df2)\\n    >>> %timeit corrstats.rolling_corr(10)  # (2)!\\n    12 ms Â± 50.5 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)\\n    \\n\\n  1. Using Pandas\\n  2. Using our class\\n\\nAnother addition we made concerns indexing. Since `obj1` and `obj2` are not\\nregular Pandas objects anymore, we cannot simply apply `pd_indexing_func` on\\nthem. Instead, we can use the method\\n[ArrayWrapper.indexing_func_meta](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.indexing_func_meta)\\nto get the rows, columns, and groups that this operation would select. We then\\napply those arrays on both NumPy objects. This approach is exceptionally\\nuseful because now we can select any data from the final shape:\\n\\n    \\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2_sh)\\n    >>> corrstats.loc[\\'2020-01-02\\':\\'2020-01-05\\'].rolling_corr(3)  # (1)!\\n                             plain            shuffled\\n                       a         b         a         b\\n    2020-01-02       NaN       NaN       NaN       NaN\\n    2020-01-03       NaN       NaN       NaN       NaN\\n    2020-01-04 -0.327327  0.327327  0.327327 -0.981981\\n    2020-01-05  0.500000 -0.240192 -0.654654 -0.960769\\n    \\n\\n  1. Compute the rolling correlation coefficient only between 2020-01-02 and 2020-01-05\\n\\nNote\\n\\nNot all classes support indexing on rows. To make sure you can select rows,\\ncheck whether the instance property `column_only_select` is False.\\n\\nThat\\'s how most high-tier classes in vectorbt are built. As a rule of thumb:\\n\\n  * For performance reasons, vectorbt mostly follows the following process: Pandas ![âž¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/27a1.svg) NumPy/Numba ![âž¡](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/27a1.svg) Pandas. The first stage builds a wrapper from Pandas objects, while the last stage uses the wrapper to present the results to the user.\\n  * Pandas metadata such as shape and index is accessible via the attribute `wrapper`. For example, `wrapper.ndim` returns the number of dimensions the current instance holds.\\n  * Don\\'t call classes directly. Use the class methods, which usually start with the preffix `from_`. The constructor `__init__` is most likely reserved for indexing purposes (by the way, that\\'s why we use `vbt.MA.run()` instead of `vbt.MA()`)\\n  * Wrapper allows indexing of any objects that have elements aligned per row or column, even of more complex layouts (see [Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records))\\n\\n### Base accessor\\n\\nThis subpackage also contains the accessor\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor),\\nwhich exposes many basic operations to the end user and is being subclassed by\\nall other accessors. It inherits\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nand thus we can do with it everything what we can do with our custom\\n`CorrStats` class. Why calling an accessor a \"base\" accessor? Because it is\\nthe superclass of all other accessors in vectorbt and provides them with the\\ncore combining, reshaping, and indexing functions, such as\\n[BaseAccessor.to_2d_array](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.to_2d_array)\\nto convert a Pandas object into a two-dimensional NumPy array.\\n\\nThe \"access\" to the accessor is simple:\\n\\n    \\n    \\n    >>> df.vbt\\n    <vectorbtpro.accessors.Vbt_DFAccessor at 0x7fe3c19f7d68>\\n    \\n    >>> df.vbt.to_2d_array()\\n    array([[ 0,  5, 10, 15],\\n           [ 1,  6, 11, 16],\\n           [ 2,  7, 12, 17],\\n           [ 3,  8, 13, 18],\\n           [ 4,  9, 14, 19]])\\n    \\n\\nIn the example above,\\n[Vbt_DFAccessor](https://vectorbt.pro/pvt_6739a1da/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)\\nis the main accessor for DataFrames, and as you can see in its definition,\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nappears in its superclasses.\\n\\nProbably the most interesting is the method\\n[BaseAccessor.combine](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine),\\nwhich allows for broadcasting and combining the current Pandas object with an\\narbitrary number of other array-like objects given the function `combine_func`\\n(mainly using NumPy).\\n\\n    \\n    \\n    >>> pd.Series([1, 2, 3]).vbt.combine(np.array([[4, 5, 6]]), np.add)\\n       0  1  2\\n    0  5  6  7\\n    1  6  7  8\\n    2  7  8  9\\n    \\n\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nimplements a range of\\n[unary](https://vectorbt.pro/pvt_6739a1da/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.unary_magic_config)\\nand\\n[binary](https://vectorbt.pro/pvt_6739a1da/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.binary_magic_config)\\nmagic methods using this method. For example, let\\'s invoke\\n`BaseAccessor.__add__`, which implements addition:\\n\\n    \\n    \\n    >>> pd.Series([1, 2, 3]) + np.array([[4, 5, 6]])  # (1)!\\n    ValueError: Length of values (1) does not match length of index (3)\\n    \\n    >>> pd.Series([1, 2, 3]).vbt + np.array([[4, 5, 6]])  # (2)!\\n       0  1  2\\n    0  5  6  7\\n    1  6  7  8\\n    2  7  8  9\\n    \\n\\n  1. Without `.vbt`, the addition is done by Pandas\\n  2. With `.vbt`, the addition is done by vectorbt\\n\\nAll of these magic methods were added using class decorators. There are a lot\\nof class decorators for all kind of things in vectorbt. Usually, they take a\\nconfig and attach many attributes at once in some automated way.\\n\\n## Generic\\n\\n    \\n    \\n    flowchart TD;\\n        Analyzable -->|inherits| Wrapping;\\n        Analyzable -->|inherits| StatsBuilderMixin;\\n        Analyzable -->|inherits| PlotsBuilderMixin;\\n        GenericAccessor -->|inherits| BaseAccessor;\\n        GenericAccessor -->|inherits| Analyzable;\\n\\nSubpackage [generic](https://vectorbt.pro/pvt_6739a1da/api/generic/) is the\\ncomputational core of vectorbt. It contains modules for processing and\\nplotting time series and numeric data in general. More importantly: it\\nimplements an [arsenal](https://vectorbt.pro/pvt_6739a1da/api/generic/nb/) of\\nNumba-compiled functions for accelerating and extending Pandas! Those\\nfunctions are powering many corners of vectorbt, from indicators to portfolio\\nanalysis. But for now, let\\'s focus on classes that could make our `CorrStats`\\nclass more powerful.\\n\\n### Builder mixins\\n\\nBuilder [mixins](https://en.wikipedia.org/wiki/Mixin) are classes that, once\\nsubclassed by a class, allow to build a specific functionality from this\\nclass\\' attributes. There are two prominent members:\\n[StatsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)\\nand\\n[PlotsBuilderMixin](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin).\\nThe former exposes the method\\n[StatsBuilderMixin.stats](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)\\nto compute various metrics, while the latter exposes the method\\n[PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_6739a1da/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)\\nto display various subplots. Both are subclassed by almost every class that\\ncan analyze data.\\n\\n### Analyzing\\n\\nClass\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)\\ncombines\\n[Wrapping](https://vectorbt.pro/pvt_6739a1da/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)\\nand Builder mixins. It combines everything we introduced above to build a\\nfoundation for a seamless data analysis; that\\'s why it\\'s being subclassed by\\nso many high-tier classes, such as\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)\\nand\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records).\\n\\nWhat are we waiting for? Let\\'s adapt our `CorrStats` class to become\\nanalyzable!\\n\\n    \\n    \\n    >>> class CorrStats(vbt.Analyzable):  # (1)!\\n    ...     _expected_keys = vbt.Analyzable._expected_keys | {\"obj1\", \"obj2\"}\\n    ...\\n    ...     @classmethod\\n    ...     def from_objs(cls, obj1, obj2):\\n    ...         (obj1, obj2), wrapper = vbt.broadcast(\\n    ...             obj1, obj2, \\n    ...             to_pd=False,\\n    ...             return_wrapper=True\\n    ...         )\\n    ...         return cls(wrapper, obj1, obj2)\\n    ...\\n    ...     def __init__(self, wrapper, obj1, obj2):\\n    ...         vbt.Analyzable.__init__(self, wrapper, obj1=obj1, obj2=obj2)  # (2)!\\n    ...\\n    ...         self._obj1 = vbt.to_2d_array(obj1)\\n    ...         self._obj2 = vbt.to_2d_array(obj2)\\n    ...\\n    ...     def indexing_func(self, pd_indexing_func, **kwargs):\\n    ...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)\\n    ...         new_wrapper = wrapper_meta[\"new_wrapper\"]\\n    ...         row_idxs = wrapper_meta[\"row_idxs\"]\\n    ...         col_idxs = wrapper_meta[\"col_idxs\"]\\n    ...         return self.replace(\\n    ...             wrapper=new_wrapper,\\n    ...             obj1=self.obj1[row_idxs, :][:, col_idxs],\\n    ...             obj2=self.obj2[row_idxs, :][:, col_idxs]\\n    ...         )\\n    ...\\n    ...     @property\\n    ...     def obj1(self):\\n    ...         return self._obj1\\n    ...\\n    ...     @property\\n    ...     def obj2(self):\\n    ...         return self._obj2\\n    ...\\n    ...     def corr(self):\\n    ...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)\\n    ...         return self.wrapper.wrap_reduced(out)\\n    ...\\n    ...     def rolling_corr(self, window):\\n    ...         out = vbt.nb.rolling_corr_nb(\\n    ...             self.obj1, self.obj2, \\n    ...             window, minp=window)\\n    ...         return self.wrapper.wrap(out)\\n    ...\\n    ...     _metrics = vbt.HybridConfig(  # (3)!', start_char_idx=30279, end_char_idx=44545, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.037328701543373234),\n",
       " NodeWithScore(node=TextNode(id_='7f856c6e-02ee-4d7d-bc80-c1499f8878dd', embedding=None, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a2798d13-a1e0-43f3-81e1-37ac37c69ea9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='ab0bca1ecc65736e95d97952ac19357fe7700565b65e93ba84d3fbdfff05ab57'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='23a68867-577b-45cd-8c81-324775b1b92b', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../quant_scraper/docs/vbt_pro/documentation-building-blocks.txt', 'file_name': 'documentation-building-blocks.txt', 'file_type': 'text/plain', 'file_size': 59262, 'creation_date': '2024-01-28', 'last_modified_date': '2024-01-28', 'last_accessed_date': '2024-01-28'}, hash='4a572a8f360d1d09082cb7e16863d79c1c943beb21ed4e3c034b62abc52b9ae5')}, text='...         corr=dict(\\n    ...             title=\\'Corr. Coefficient\\',\\n    ...             calc_func=\\'corr\\'\\n    ...         )\\n    ...     )\\n    ...\\n    ...     _subplots = vbt.HybridConfig(  # (4)!\\n    ...          rolling_corr=dict(\\n    ...              title=vbt.Sub(\"Rolling Corr. Coefficient (window=$window)\"),\\n    ...              plot_func=vbt.Sub(\\'rolling_corr($window).vbt.plot\\'),  # (5)!\\n    ...              pass_trace_names=False\\n    ...          )\\n    ...     )\\n    \\n\\n  1. Replace `Wrapping` with `Analyzable`\\n  2. Replace `Wrapping` with `Analyzable`\\n  3. Define default metrics for [StatsBuilderMixin.stats](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)\\n  4. Define default subplots for [PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_6739a1da/api/generic/stats_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)\\n  5. Recall Templating and Attribute resolution?\\n\\nWe changed a few things: replaced `Wrapping` with `Analyzable`, and added some\\nmetrics and subplots based on `CorrStats.corr` and `CorrStats.rolling_corr`.\\nThat\\'s it! We can now pass arbitrary array-like objects to\\n`CorrStats.from_objs` and it will return an instance that can be used to\\nanalyze the correlation between the objects, in particular using\\n`CorrStats.stats` and `CorrStats.plots`:\\n\\n    \\n    \\n    >>> corrstats = CorrStats.from_objs(df1, df2)\\n    >>> corrstats.stats(column=\\'a\\')  # (1)!\\n    Corr. Coefficient    0.3\\n    Name: a, dtype: object\\n    \\n    >>> corrstats[\\'a\\'].stats()  # (2)!\\n    Corr. Coefficient    0.3\\n    Name: a, dtype: object\\n    \\n    >>> corrstats.plots(template_context=dict(window=3)).show()  # (3)!\\n    \\n\\n  1. Compute metrics for all columns and display only `a`\\n  2. Compute and display metrics only for `a`\\n  3. Set the rolling window to 3 and display subplots for all columns\\n\\n![](https://vectorbt.pro/pvt_6739a1da/assets/images/documentation/bblocks/analyzing.svg)\\n\\nThere is nothing more satisfying than not having to write boilerplate code.\\nThanks to\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),\\nwe can shift our focus entirely to analysis, while vectorbt takes care of\\neverything else.\\n\\n### Generic accessor\\n\\nWe don\\'t have to look far to find a class that inherits\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable):\\nclass\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)\\nextends the class\\n[BaseAccessor](https://vectorbt.pro/pvt_6739a1da/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)\\nto deliver statistics and plots for any numeric data. It\\'s a size-that-fits-\\nall class with an objective to replicate, accelerate, and extend Pandas\\' core\\nfunctionality. It implements in-house rolling, mapping, reducing, splitting,\\nplotting, and many other kinds of methods, which can be used on any Series or\\nDataFrame in the wild.\\n\\nIn a nutshell,\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)\\ndoes the following:\\n\\n  * Wraps many Numba-compiled functions from [nb](https://vectorbt.pro/pvt_6739a1da/api/generic/nb/) module and offers them as methods that mimic some of the Pandas most popular functions. Some of them have meta versions (accepting UDFs that take metadata instead of arrays) or can work on grouped data.\\n  * Wraps many data transformation utilities from [scikit-learn](https://scikit-learn.org/stable/)\\n  * Displays a range of statistics similar to [pandas.DataFrame.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)\\n  * Wraps widgets from [plotting](https://vectorbt.pro/pvt_6739a1da/api/generic/plotting/) and provides many custom plotting methods. Some of them even support interactive controls for analyzing groups of data.\\n  * Extracts basic events in form of [Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records), such as drawdowns\\n\\nSimilarly to the Base accessor, the generic accessor uses [class\\ndecorators](https://vectorbt.pro/pvt_6739a1da/api/generic/decorators/) and\\n[configs](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.nb_config)\\nto attach many Numba-compiled and scikit-learn functions at once.\\n\\nUsage is similar to `CorrStats`, except that we can call the generic accessor\\ndirectly on Pandas objects since it\\'s being directly subclassed by\\n[Vbt_DFAccessor](https://vectorbt.pro/pvt_6739a1da/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)!\\n\\n    \\n    \\n    >>> df.vbt.stats(column=\\'a\\')\\n    Start        2020-01-01 00:00:00\\n    End          2020-01-05 00:00:00\\n    Period           5 days 00:00:00\\n    Count                          5\\n    Mean                         2.0\\n    Std                     1.581139\\n    Min                            0\\n    Median                       2.0\\n    Max                            4\\n    Min Index    2020-01-01 00:00:00\\n    Max Index    2020-01-05 00:00:00\\n    Name: a, dtype: object\\n    \\n\\n## Records\\n\\n    \\n    \\n    flowchart TD;\\n        Records -.->|references| ColumnMapper;\\n        Records -.->|references| MappedArray;\\n        Records -->|inherits| Analyzable;\\n        MappedArray -.->|references| ColumnMapper;\\n        MappedArray -->|inherits| Analyzable;\\n\\nRecords are [structured\\narrays](https://numpy.org/doc/stable/user/basics.rec.html) \\\\- a NumPy array\\nthat can hold different data types, just like a Pandas DataFrame. Records have\\none big advantage over DataFrames though: they are well understood by Numba,\\nthus we can generate and use them efficiently. So, what\\'s the catch? Records\\nhave no (index) labels and the API is very limited. We also\\n[learned](https://vectorbt.pro/pvt_6739a1da/documentation/fundamentals/) that\\nvectorbt doesn\\'t like heterogenous data and prefers to work with multiple\\nhomogeneous arrays (remember how we need to split OHLC into O, H, L, and C?).\\nNevertheless, they take a very important place in our ecosystem - as\\ncontainers for event data.\\n\\nTrading is all about events: executing trades, aggregating them into\\npositions, analyzing drawdowns, and much more. Each of such events is a\\ncomplex piece of data that needs a container optimized for fast writes and\\nreads, especially inside a Numba-compiled code (but please not a list of\\ndictionaries, they are **very inefficient** ). Structured arrays is exactly\\nthe data structure we need! Each event is a record that holds all the required\\ninformation, such the column and row where it originally happened.\\n\\nBecause structured arrays are hard to analyze, there is a class just for this\\n-\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)!\\nBy subclassing\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)\\n(suprise, surprise), it wraps a structured NumPy array and provides us with\\nuseful tools for its analysis. Every\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)\\ninstance can be indexed just like a regular Pandas object and compute various\\nmetrics and plot graphs.\\n\\nLet\\'s generate the\\n[Drawdowns](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)\\nrecords for two columns of time series data:\\n\\n    \\n    \\n    >>> dd_df = pd.DataFrame({\\n    ...     \\'a\\': [10, 11, 12, 11, 12, 13, 12],\\n    ...     \\'b\\': [14, 13, 12, 11, 12, 13, 14]\\n    ... }, index=[datetime(2020, 1, 1) + timedelta(days=i) for i in range(7)])\\n    >>> drawdowns = dd_df.vbt.drawdowns\\n    >>> drawdowns.records_readable\\n       Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\\\\\\\\\n    0            0      a     2020-01-03      2020-01-04       2020-01-04   \\n    1            1      a     2020-01-06      2020-01-07       2020-01-07   \\n    2            0      b     2020-01-01      2020-01-02       2020-01-04   \\n    \\n      End Timestamp  Peak Value  Valley Value  End Value     Status  \\n    0    2020-01-05        12.0          11.0       12.0  Recovered  \\n    1    2020-01-07        13.0          12.0       12.0     Active  \\n    2    2020-01-07        14.0          11.0       14.0  Recovered \\n    \\n    >>> drawdowns[\\'b\\'].records_readable  # (1)!\\n       Drawdown Id Column Peak Timestamp Start Timestamp Valley Timestamp  \\\\\\\\\\n    0            0      b     2020-01-01      2020-01-02       2020-01-04   \\n    \\n      End Timestamp  Peak Value  Valley Value  End Value     Status  \\n    0    2020-01-07        14.0          11.0       14.0  Recovered\\n    \\n\\n  1. Select all records of the column `b` and display them in a human-readable format\\n\\nThat\\'s a lot of information! Each field is a regular NumPy array, so where do\\nwe get this rich information from? Maybe to your surprise, but the labels of\\nthe DataFrames above were auto-generated from the metadata that\\n[Drawdowns](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)\\nholds. This metadata is called a \"field config\" - a regular\\n[Config](https://vectorbt.pro/pvt_6739a1da/api/utils/config/#vectorbtpro.utils.config.Config)\\nthat describes each field (for instance,\\n[Drawdowns.field_config](https://vectorbt.pro/pvt_6739a1da/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns.field_config)).\\nThis makes possible automating and enhancing the behavior of each field. Class\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records),\\nthe base class to all records classes, has many methods to read and interpret\\nthis config.\\n\\n    \\n    \\n    >>> drawdowns.status.values  # (1)!\\n    array([1, 0, 1])\\n    \\n    >>> drawdowns.get_apply_mapping_arr(\\'status\\')  # (2)!\\n    array([\\'Recovered\\', \\'Active\\', \\'Recovered\\'], dtype=object)\\n    \\n\\n  1. Raw data\\n  2. Data enhanced using the field config\\n\\n### Column mapper\\n\\nRecords are one-dimensional structured NumPy arrays. Records from multiple\\ncolumns are concatenated into a single array, so we need a mechanism to group\\nthem by column or group, for instance, to aggregate values column-wise. This\\nis a non-trivial task because finding the records that correspond to a\\nspecific column requires searching all records, which is slow when done\\nrepeatedly. The task of the class\\n[ColumnMapper](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper)\\nis to index all columns only once and cache the result (see\\n[ColumnMapper.col_map](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper.col_map)).\\nA column mapper has at least two more advantages: it allows for grouping\\ncolumns and enables efficient Indexing.\\n\\n    \\n    \\n    >>> drawdowns.col_mapper.col_map  # (1)!\\n    (array([0, 1, 2]), array([2, 1]))\\n    \\n\\n  1. An array with the indices of records ordered by column, and an array with the number of records per column\\n\\nThe produced column map means that the column `a` has two records at indices 0\\nand 1, while the column `b` is represented by only one record at index 2.\\n\\n### Mapped arrays\\n\\nIf\\n[Records](https://vectorbt.pro/pvt_6739a1da/api/records/base/#vectorbtpro.records.base.Records)\\nis our own DataFrame for events, then\\n[MappedArray](https://vectorbt.pro/pvt_6739a1da/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray)\\nis our own Series! Each field in records can be mapped into a _mapped_ array.\\nIn fact, a mapped array is where most calculations take place. It\\'s just like\\n[GenericAccessor](https://vectorbt.pro/pvt_6739a1da/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor),\\nbut with a totally different representation of data: one-dimensional and\\nclustered instead of two-dimensional and column-wise. We can even seemingly\\nconvert between both representations. Why wouldn\\'t we then simply convert a\\nmapped array into a regular Series and do all the analysis there? There are\\nseveral reasons:\\n\\n  1. Event data is usually sparse: If 1,000,000 data points produced 50 events, it\\'s much faster to analyze 50 values then having to convert them back and deal with 9,999,950 NaNs.\\n  2. Multiple events, such as orders, can happen within the same bar, which cannot be represented in Pandas efficiently\\n\\nLet\\'s analyze the drawdown values of `drawdowns`:\\n\\n    \\n    \\n    >>> dd_ma = drawdowns.drawdown\\n    >>> dd_ma\\n    <vectorbtpro.records.mapped_array.MappedArray at 0x7ff6d8514f98>\\n    \\n    >>> dd_ma.values  # (1)!\\n    array([-0.08333333, -0.07692308, -0.21428571])\\n    \\n    >>> dd_ma.stats(column=\\'a\\')\\n    Start        2020-01-01 00:00:00\\n    End          2020-01-07 00:00:00\\n    Period           7 days 00:00:00\\n    Count                          2\\n    Mean                   -0.080128\\n    Std                     0.004533\\n    Min                    -0.083333\\n    Median                 -0.080128\\n    Max                    -0.076923\\n    Min Index    2020-01-05 00:00:00\\n    Max Index    2020-01-07 00:00:00\\n    Name: a, dtype: object\\n    \\n    >>> dd_ma.to_pd()  # (2)!\\n                       a         b\\n    2020-01-01       NaN       NaN\\n    2020-01-02       NaN       NaN\\n    2020-01-03       NaN       NaN\\n    2020-01-04       NaN       NaN\\n    2020-01-05 -0.083333       NaN\\n    2020-01-06       NaN       NaN\\n    2020-01-07 -0.076923 -0.214286\\n    \\n\\n  1. Holds three values: two from `a` and one from `b`\\n  2. Can be converted into Pandas, but at what cost?\\n\\nThanks to\\n[ColumnMapper](https://vectorbt.pro/pvt_6739a1da/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper)\\nand\\n[Analyzable](https://vectorbt.pro/pvt_6739a1da/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),\\nwe can select rows and columns from a mapped array the same way as from\\nrecords or any regular Pandas object:\\n\\n    \\n    \\n    >>> dd_ma[\\'b\\'].values\\n    array([-0.21428571])\\n    \\n\\n## Summary\\n\\nKudos for following me all the way down here! The classes that we just covered\\nbuild a strong foundation for data analysis with vectorbt; they implement\\ndesign patterns that are encountered in most other places across the codebase,\\nwhich makes them very easy to recognize and extend. In fact, the most hard-\\ncore class\\n[Portfolio](https://vectorbt.pro/pvt_6739a1da/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)\\nis very similar to our `CorrStats`.\\n\\nYou\\'re now more than ready for using vectorbt, soldier\\n![ðŸŒŸ](https://cdn.jsdelivr.net/gh/jdecked/twemoji@14.1.2/assets/svg/1f31f.svg)\\n\\n[ Python\\ncode](https://vectorbt.pro/pvt_6739a1da/assets/jupytext/documentation/building-\\nblocks.py.txt)', start_char_idx=44550, end_char_idx=59233, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.036965438342354044)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine2 = index2.as_chat_engine(chat_mode=\"best\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: In my strategy I have 4 take profits and stop loss. How can I move stop loss to breakeven after the first take profit is hit?\n",
      "To move the stop loss to breakeven after the first take profit is hit in your strategy with 4 take profits and a stop loss, you can follow these steps:\n",
      "\n",
      "1. Set your initial stop loss level at a certain distance from your entry price.\n",
      "2. When the price reaches your first take profit level, adjust your stop loss level to your entry price (breakeven).\n",
      "3. Continue to monitor the price movement and adjust your remaining take profit levels accordingly.\n",
      "\n",
      "Here's an example to illustrate this strategy:\n",
      "\n",
      "Let's say you enter a trade with a stop loss of $100 and four take profit levels: TP1 at $50, TP2 at $100, TP3 at $150, and TP4 at $200.\n",
      "\n",
      "1. Set your initial stop loss level at $100.\n",
      "2. When the price reaches TP1 at $50, adjust your stop loss level to your entry price (breakeven), which is $100.\n",
      "3. If the price continues to move in your favor and reaches TP2 at $100, adjust your stop loss level to TP1 ($50).\n",
      "4. If the price reaches TP3 at $150, adjust your stop loss level to TP2 ($100).\n",
      "5. Finally, if the price reaches TP4 at $200, adjust your stop loss level to TP3 ($150).\n",
      "\n",
      "By moving your stop loss to breakeven after the first take profit is hit, you ensure that you lock in some profits and reduce the risk of losing money on the trade. Additionally, adjusting your stop loss levels as the price reaches each take profit level allows you to protect more profits along the way.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine2.chat(\"In my strategy I have 4 take profits and stop loss. How can I move stop loss to breakeven after the first take profit is hit?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
